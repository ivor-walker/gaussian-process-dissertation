\section{Introduction}
% Before starting with the actual definitions and specifics, you could have 1-2 pages of material where you explain the broad context of the project and its aims.

\paragraph{Defining Gaussian processes}
Most statistical methods describe relationships between data by estimating weights that linearly combine the data to produce an output. This is fundamentally different to Gaussian processes (GPs), which describe a distribution over possible functions that describe the data. We aim to describe what Gaussian processes are in terms of familiar weight-based statistical tools, namely Bayesian regression. We start with an in-depth look into Bayesian linear regression, relating it to frequentist approaches and deriving important results such as the the weight and predictive distributions. By applying the linear model to some non-linear flexible function of the data $\phi(X)$, we make the model more flexible. Then, we generalise our Bayesian linear regression in terms of a distribution of possible functions that $\phi$ could be, and translate our Bayesian linear regression into a Gaussian process. We finish by investigating general properties of Gaussian processes that are analagous to properties of Bayesian linear regression, such as predictive distributions and log-likelihoods, and describe some practical considerations involved in their usage.

% One can think of a Gaussian process as defining a distribution over functions, and inference taking place directly in the space of functions, the function-space two equivalent views view. Although this view is appealing it may initially be difficult to grasp, so we start our exposition in section 2.1 with the equivalent weight-space view which may be more familiar and accessible to many, and continue in section 2.2 with the function-space view. Gaussian processes often have characteristics that can be changed by setting certain parameters and in section 2.3 we discuss how the properties change as these parameters are varied. The predictions from a GP model take the form of a full predictive distribution.

\paragraph{Covariance functions}
The behaviour of the covariance function and its resulting covariance matrix is crucial to understanding the behaviour of a Gaussian process - the covariance matrix appears in every term of the predictive distribution and the marginal likelihood. In particular, the choice of covariance function affects the smoothness of the overall GP. We describe the mechanisms that the covariance function has at its disposal to vary smoothness, namely its degree of differentiability and the choice of length scale. We investigate in detail the stationary family of covariance functions, and develop a new tool to unify differentiability and length scale into a single notion of smoothness. Finally, we introduce the most widespread stationary covariance functions, their properties, and apply them to toy data to illustrate their behaviour.

% We have seen that a covariance function is the crucial ingredient in a Gaussian process predictor, as it encodes our assumptions about the function which we wish to learn. From a slightly different viewpoint it is clear that in supervised learning the notion of similarity between data points is crucial; it is a basic similarity assumption that points with inputs x which are close are likely to have similar target values y, and thus training points that are near to a test point should be informative about the prediction at that point. Under the Gaussian process view it is the covariance function that defines nearness or similarity
% An arbitrary function of input pairs x and x′ will not, in general, be a valid valid covariance functionscovariance function.1 The purpose of this chapter is to give examples of some commonly-used covariance functions and to examine their properties. Section 4.1 defines a number of basic terms relating to covariance functions. Section 4.2 gives examples of stationary, dot-product, and other non-stationary covariance functions, and also gives some ways to make new ones from old. Section 4.3 introduces the important topic of eigenfunction analysis of covariance functions, and states Mercer's theorem which allows us to express the covariance function (under certain conditions) in terms of its eigenfunctions and eigenvalues. 

\paragraph{Computational concerns}
Training and performing inference on a Gaussian processes requires inverting an $n \times n$ matrix $[K(X,X) + \sigma^2_n]$. Naively inverting this matrix $X$ has a computational complexity of $O(n^3)$. Unfortunately, this cubic complexity is shared by most covariance functions and renders Gaussian processes unusable in practice - a covariance matrix with $n = 1000$ would require over one billion operations to invert naively, and this cost would need to be paid for each hyperparameter optimisation step (epoch) and each prediction. One approach is to approximate this inversion for any covariance matrix. These strategies for any covariance function fall into two categories: those that produce a single approximation for the entire dataset, or those that produce several approximations that are "experts" in a particular region of the dataset and combine these local approximations to form a global approximation. We focus on global approximations because the local-expert methods are much newer and lack robust open-source implementations. Another approach is to design a specific covariance function that produces a covariance matrix which has certain exploitable properties that allow for much quicker and more accurate inversions, at the cost of flexibility. We introduce one structured approach, the celerite model, and discuss the origins of these tradeoffs in detail.

\paragraph{Case study: SED residuals in astrophysics}
The case study applies these ideas to SED residual modelling in astrophysics. Contemporary SED models can exhibit "low-frequency wobbles" in their residualsthat violate the assumption of zero-mean noise used for inference on physical parameters. Because the source of these wobbles is not well understood and their scale varies across galaxies, we need a flexible, probabilistic filter that can remove such correlated noise without causing identifiability issues by touching the regular noise that downstream inference relies on. This is a task that GPs are well suited for. We evaluate the two most performant approximation methods on SED residuals modelling: a Stochastic Variational GP (SVGP) that trades exactness and completeness for the ability to use the preferred SE kernel, and a celerite model that is faster and more accurate at the price of being tied to an inferior Matern 3/2 kernel. The central question is whether the dramatic speed gains from celerite outweigh the fidelity loss when the rougher Matérn 3/2 kernel replaces SE in a task that ideally prefers SE-like smoothness. Wall-clock results demonstrate that SVGP required orders-of-magnitude more computation to converge than the celerite setup, and is generally infeasible for the size of datasets being analysed. At the same time, we show that Matern 3/2 overfits and causes identifiability issues by tracking the regular noise too closely, while SE avoids modelling them and captures broad trends. We conclude by developing and reviewing several strategies for addressing SVGP's computational shortcomings, adapting Matern 3/2 to preserve identifiability whilst retaining celerite's speed, and alternative structured approximation methods.
