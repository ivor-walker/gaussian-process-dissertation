\section{Conclusion}

\paragraph{Analysis of covariance functions}
Conceptually, reframing smoothness as a single lens that unites length-scale, mean-square (MS) differentiability, and spectral decay proved useful. The Taylor–upcrossing analysis shows how length-scale governs the density of wiggles in typical function draws; shorter scales imply more upcrossings, while longer scales enforce broader, slower variations. In MS space, continuity and differentiability conditions translate directly into constraints on the kernel. For stationary kernels the spectral viewpoint makes the trade visible: fast high-frequency spectral decay yields very smooth sample paths. Together, these perspectives clarify why ostensibly similar kernels behave so differently downstream and proved fundamental in understanding the different performances of Matern 3/2 versus SE in the case study.

\paragraph{Computational efficiency versus generality}
The work established and disentangled several approaches to making GPs computationally tractable. Stochastic variational GPs (SVGP) buy generality at the price of accuracy and wall-clock time, while structured methods, exemplified by celerite, achieve near-exact linear-time algebra on restricted kernel classes and data layouts. Even though SVGP had a theoretically attractive computational complexity, its training took 65,000 times longer than celerite training and ultimately was too computationally infeasible to recommend for this application. In this case study, practical cost depended more on optimiser dynamics and noise levels than on theoretical computational complexity.

The case study's objective was to remove correlated, low-frequency "wobbles" while leaving high-frequency noise intact so downstream inference remains valid. SE's rapid spectral decay matches that need by resisting high-frequency noise but rising and falling to capture correlated residuals. By contrast, Matérn 3/2's rougher prior tends to pass through more datapoints and, in this setting, overfits both low-frequency and high-frequency structure. 

These tradeoffs exposed a central tension between efficiency and generality that exists in GP modelling. SE avoided enough high-frequency noise to be identifiable, but required a generalised approximation like SVGP that was not computationally feasible. Celerite was computationally feasible for the case study, but it could only work with Matern 3/2 which overfit to the high-frequency noise and produced unidentifiable results.

There are three paths to addressing this tradeoff. Firstly, recent software developments have enabled generalised frameworks such as SVGP to run on GPUs, which could make generalised models practically feasible without needing to address their complexity issues. Although similar successes have been achieved with neural networks, this approach relies on GPUs which are themselves expensive and not always available and do not address the fundamental complexity issues that generalised frameworks have. Secondly, the existing structured approximations can be manipulated to approximate other covariance functions, e.g. Leung's approximation of SE with celerite \cite{galaxy-gp-noise}. However, these extension methods produce approximations that sometimes differ from the target of the approximation. For example, Leung found that celerite's approximation of SE produces functions that are more flexible at the tails than SE, which is an issue for applications that require perfect smoothness like SED residual modelling. 

\paragraph{New structured approximations}
The most promising path forward is via new structured approximation methods that retain dimensionality restrictions but can produce more classes of covariance functions. One example of a newer structured approximation framework is "structured kernel interpolation" (SKI) \cite{ski}, which has been shown to approximate a wide range of covariance functions with linear complexity in similar one-dimensional time series applications. The major drawbacks are its tendancy to produce discontinuous predictions and its reliance on inducing points - our case study demonstrated that inducing point methods are less reliable in high noise scenarios such as SED residual modelling. Evans and Nair \cite{ski-smoother} exploited a different structure in the covariance matrix rather than using inducing points to overcome these discontinuities, and this technique could readily be adapted to the case study's setting.
