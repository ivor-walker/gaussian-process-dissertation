\section{Defining the Gaussian Process}

\subsection{Weight-space view}

\subsubsection{Standard linear model}
The standard linear model summises that we have some data-generating function $f(.)$ that linearily combines training data $X$, parameters of some model $W$, and some irreduceable noise $\epsilon$ (because $y$ is rarely a perfect observation of $f(X)$, e.g. if $y$ is measured inaccurately) to produce an output $y$:
\begin{equation} \label{eq:linear_model}
    \begin{aligned}
        f(X) = X^T W \\
        y = f(X) + \epsilon \\
    \end{aligned}
\end{equation}

\paragraph{Errors}
 The standard linear model assumes that our irreduceable noise, or errors, $\epsilon$ are drawn from a Gaussian distribution:
\begin{equation} \label{eq:model_error_assumption}
    \epsilon \sim N(0, \sigma_n^2I)
\end{equation}
We assume our model has zero mean because all the information we have about the data-generating function $f(X)$ is contained in the weights $W$ and the data $X$. We add a covariance matrix $I$ to describe how the noise for one observation is related to the noise of another observation. In this case, we assume that the noise is independent and identically distributed (i.i.d.) across all observations, so $I$ is an "identity matrix" where each diagonal element is 1 and all off-diagonal elements are 0. The variance $\sigma_n^2$ describes how much noise we expect in our observations.

We can combine our expressions for the data-generating function and and assumptions about our noise to produce a conditional distribution that fully describes $y$, also known as the likelihood of $y$ given $X$ and $W$:
% michael: dist of errors is a little confusing, within the rest of the sentence
\begin{equation*}
    p(y|X,W) = \mathcal{N}(X^TW, \sigma^2_nI)
\end{equation*}
% This is the distribution from which $y$ is drawn from after perfect knowledge of our data and the weights of our data-generating function. It effectively specifies a distribution of errors - the difference between the expected value of $y$ given $X$ and $W$ and the actual value of $y$.

\subsubsection{Determining weights}
Our first task is to estimate the weights for our data-generating function $W$, as we typically do not know these in advance. Frequentist approaches focus on arriving at a single estimate of these weights ($\hat{W}$) via "maximum likelihood estimation" (MLE), whereas Bayesian approaches treat $W$ as a random variable for which a distribution is specified.

\paragraph{Frequentist regression}
Because our likelihood $p(y|X,W)$ is Gaussian, it is at its highest density around the expected value $X^TW$ and we can use iterative optimisation routines to find a single estimate for $W$, $\hat{W}$, that maximises our likelihood. Because we assume $E[\epsilon] = 0$ \ref{eq:model_error_assumption}, the values of $W$ at the maximum of $p(y|X,W)$ are also the values of $W$ that minimise the squared error $||y - X^TW||^2$. We can communicate uncertainty surrounding our estimations of $W$ by computing "standard errors", or the ratio between the variance of errors and the variance in $X$. A high variance in errors shows that the variation in $y$ is more noise than signal captured by $X$, but a broader range of $X$ makes estimating $W$ easier.

\paragraph{Bayesian regression}
Instead of producing single point estimates for weights and uncertainty in estimating them, Bayesian statistics treats $W$ as a random variable and specifies an expected value and a variance. Placing $W$ in a probabilistic framework allows us to propagate uncertainty throughout the model and to encode beliefs (e.g. from domain experts) about the weights before observing the data.

We start with a "prior" distribution of $W$, which the Bayesian linear model assumes:
\begin{equation} \label{eq:prior_distribution}
    p(W) = N(0, \Sigma_p)
\end{equation}
% michael: just one possible prior
Then, we observe the data and update our beliefs about the weights using Bayes' theorem to produce a "posterior" distribution $p(W|X,y)$:
\begin{equation*}
    p(W|X,y) = \frac{p(y|X,W)p(W)}{p(y|X)}
\end{equation*}
$p(y|X,W)$ is the density of the residuals after applying $p(W)$ to $X,W$ under our assumed noise model $\epsilon$, and $p(y|X)$ is our likelihood.

\paragraph{Deriving our posterior}
To understand the relationship between $p(W|X,y)$ and $W$, we can ignore terms that do not vary with $W$ (e.g. our marginal likelihood) by absorbing them into a proportionality constant:
\begin{equation} \label{eq:posterior}
    p(W|X, y) \propto p(y|X,W)p(W)
\end{equation}

% We can write $p(Y|X,W)$ as the distribution of errors for each datapoint $i$:
% \begin{equation*}
%     p(y|X,W) = \prod_{i=1}^N \mathcal{N}(y_i|X^TW, \sigma^2_n)
% \end{equation*}

The probability density function (PDF) measures probability per unit length. Integrating the PDF over a range gives us the probability of observing a value in that range. The PDF of a Gaussian distribution is given by:
\begin{equation} \label{eq:gaussian_pdf}
    N(x | \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} exp\left(-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x- \mu)\right)
\end{equation}

We can get a PDF for our likelihood by representing our errors in squared error form and substituting it into the Gaussian PDF \ref{eq:gaussian_pdf}:
\begin{equation} \label{eq:likelihood}
    p(y|X,W) = exp\left(-\frac{1}{2\sigma^2_n}||y -X^TW||^2\right)
\end{equation}

Reframing $p(W)$ as a PDF:
 \begin{equation*}
     p(W) = \frac{1}{[\sqrt{\sigma_p}]\sqrt{2\pi}} exp\left(-\frac{1}{2}\frac{([W]-[0])}{[\Sigma_p]}\right)
\end{equation*}

The first term can be absorbed into the proportionality constant. Rewriting the second term as a negative exponential:
\begin{equation} \label{eq:prior_pdf}
    p(W) \propto exp\left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)
\end{equation}

Putting both expressions for \ref{eq:likelihood} and \ref{eq:prior_pdf} into \ref{eq:posterior}:
\begin{equation*}
    p(W|X,y) \propto exp\left(-\frac{1}{2\sigma^2_n}||y -X^TW||^2\right)exp\left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)
\end{equation*}

Expanding $||y - X^TW||^2$ to $y^Ty - 2y^TXW + W^TX^TXW$:
\begin{equation*}
    p(W|X,y) \propto exp\left(-\frac{1}{2\sigma^2_n}(y^Ty - 2y^TXW + W^TX^TXW)\right)exp\left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)
\end{equation*}

Putting both exponentials together by adding their powers:
\begin{equation*}
    p(W|X,y) \propto exp\left(\frac{1}{\sigma^2_n}(y^Ty - 2y^TXW + W^TX^TXW) + \left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)\right)
\end{equation*}

Rearranging the inside term to be a quadratic, linear and constant term in $W$:
\begin{equation*}
    p(W|X,y) \propto exp\left(\frac{1}{2}W^T\left(\frac{1}{\sigma^2_n}X^TX + \Sigma_p^{-1}\right)W - \left(\frac{1}{\sigma^2_n}y^TX\right)W + \frac{1}{2}y^Ty\right)
\end{equation*}

We can ignore the constant final term. Introducing these terms to simplify this result:
\begin{equation} \label{eq:A_b}
    \begin{aligned}
        A = \Sigma_p^{-1} + \frac{1}{\sigma^2_n}X^TX \\
        b = \frac{1}{\sigma^2_n}y^TX \\
        p(W|X,y) \propto exp\left(-\frac{1}{2}W^TAW + b^TW\right) \\
    \end{aligned}
\end{equation}

\paragraph{Deriving the properties of the posterior by completing the square}
Now we have a simplified form for the posterior's PDF, we need to get it into a Gaussian form to recover the properties of the posterior distribution.

Bringing all terms inside the exponential to a single term:
\begin{equation*}
    -\frac{1}{2}W^TAW + b^TW = \frac{1}{2}\left(-W^TAW + 2b^TW\right)
\end{equation*}

Completing the square on our new inner term $W^TAW - 2b^TW$ 
\begin{equation} \label{eq:posterior_pdf}
    \begin{aligned}
        W^TAW - 2b^TW = (W - A^{-1}b)^TA(W - A^{-1}b) - b^TA^{-1}b
        p(W|X,y) \propto exp\left(-\frac{1}{2}\left((W - A^{-1}b)^TA(W - A^{-1}b) - b^TA^{-1}b\right)\right)
    \end{aligned}
\end{equation}

Our expression lines up with the Gaussian PDF's "kernel" term $exp\left(-\frac{1}{2}(W - \mu)^T\Sigma^{-1}(W - \mu)\right)$, where $\mu = A^{-1}b$ and $\Sigma = A^{-1}$ ($\Sigma^{-1} = A$). Therefore, \ref{eq:posterior_pdf} can be represented as a Gaussian distribution:
\begin{equation} \label{eq:posterior_gaussian}
    p(W|X,y) \sim N(A^{-1}b, A^{-1})
\end{equation}

Inside our definition of $A$ at $\ref{eq:A_b}$, we are missing an expression for $\Sigma_p$. Assuming independence of noise under the linear model \ref{eq:model_error_assumption}, our weight variance $\Sigma_p$ under the Bayesian linear model is "isotropic", meaning it is the same in all directions.
\begin{equation*}
    \Sigma_p = \tau^2 I
\end{equation*}
$I$ is our familiar identity matrix and $\tau^2$ is a scalar variance term, chosen as a prior.

Substituting the isotropic prior $\Sigma_p$ into $A$:
\begin{equation*}
    A = \Sigma_p^{-1} + \frac{1}{\sigma^2_n}X^TX = \left[{\tau^2}I\right]^{-1} + \frac{1}{\sigma^2_n}X^TX =  \frac{1}{\tau^2}I + \frac{1}{\sigma^2_n}X^TX
\end{equation*}

Simplifying:
\begin{equation} \label{eq:A_isotropic}
A = \frac{1}{\sigma_n^2}\left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)
\end{equation}
% michael: This detailed derivation is fine for me, as it is not part of what we teach. Please be careful that the maths are correct.

\paragraph{Gaussian posteriors and ridge regression}
So far we have worked exclusively within the Bayesian paradigm, but we can draw some value of $W$ from our posterior distribution to relate it to a frequentist framework. For Gaussian posteriors, our expected value of $W$ $A^{-1}b$ is also its mode. This is called the maximum a posteriori (MAP) estimate of W, and is due to symmetries in linear model and posterior and is not the case in general.  Our MAP estimate does not matter within the Bayesian framework but is equivelant to our frequentist $\hat{W}$.

Substituting our full expressions for $A$ \ref{eq:A_isotropic} and $b$ \ref{eq:A_b} into our MAP estimation:
\begin{equation*} 
    W_{\text{MAP}} = A^{-1}b = \left[\frac{1}{\sigma_n^2}(X^TX + \frac{\sigma_n^2}{\tau^2}I\right]^{-1} \cdot \left[\frac{1}{\sigma_n^2}y^TX\right] \\
\end{equation*}

Inverting LHS term of $A$:
\begin{equation*}
    A^{-1} = \frac{\sigma_n^2}{X^TX + \frac{\sigma_n^2}{\tau^2}I} = \sigma_n^2\left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)^{-1}
\end{equation*}

Substituting this back into $W_\text{MAP}$ cancels out the $\sigma_n^2$ term in A with the $\frac{1}{\sigma_n^2}$ term in B:
\begin{equation*}
    W_{\text{MAP}} = \sigma_n^2\left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)^{-1} \cdot \frac{1}{\sigma_n^2}y^TX = \left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)^{-1} \cdot y^TX
\end{equation*}

This is equivelant to the solution to ridge regression, where $\lambda = \frac{\sigma_n^2}{\tau^2}$.
\begin{equation*}
    W_{\text{ridge}} = \left(X^TX + \lambda I\right)^{-1}X^Ty
\end{equation*}

Ridge regression introduces some bias to lower the variance in a frequentist linear model by shrinking weights, where the $\lambda$ term controls the amount of shrinkage applied to the weights. This is traditionally useful where variance is particularly high (e.g. multicollinearity) and can be reduced at the cost of little bias.  

Our MAP estimation in Bayesian linear regression with isotropic priors is equivalent to ridge regression, where the amount of bias we introduce depends on our confidence in our priors. The more we trust our prior, the higher our $\lambda$ and the more we shrink our weights towards zero. A lower $\tau$ means we are more confident in $p(W)$ and have better priors, whereas a higher $\sigma_n$ means lower confidence in $p(y|X,W)$ and worse weights that should be shrunk closer to zero.
        
\subsubsection{Predictive distribution}
\paragraph{Deriving the predictive distribution}
Our second task is to make predictions $y_*$ using new input data $X_*$ and our previously learned weights $W$. Frequentist methods simply multiply $\hat{W}$ by $X_*$, but this does not propagate uncertainty in $W$. In this Bayesian framework, we form a "predictive distribution" which we sample from to get our noise-free function evaluations $f(X_*)$ (denoted $f_*$) and add $\epsilon$ to get our noisy predictions $y_*$.

\begin{equation*}
    p(f_*|X_*,X,y) = \int p(f_*|X_*,W) \cdot p(W | X,y)dW
\end{equation*}
$p(f_*|X_*,W)$ is what we think the function looks like after producing a prediction using $X_*$ and perfect knowledge of $W$. $p(W|X,y)$ is our familiar \ref{eq:posterior_gaussian} posterior distribution of weights. $p(f_*|X_*,W) \cdot p(W|X,y)$ is the joint distribution of our predictions and our posterior weights, which gets us the conditional distribution $p(f_*,W|X_*,X,y)$ by definition of conditional probability. Because $p(f_*,W|X_*,X,y)$ relies on our perfect knowledge of $W$, which we lack, we integrate over all possible $W$ to get the final predictive distribution $p(f_*|X_*,X,y)$
    
$p(f_*|X_*,W)$ is our error distribution, which we assume to be distributed normally and independently with our $I$ identity matrix:
\begin{equation*}
    p(f_* | X_*, W) = \mathcal{N}(f_* | W^TX_*, \sigma^2_nI)
\end{equation*}

Substituting into \ref{eq:gaussian_pdf} and absorbing the LHS term into the proportionality constant:
\begin{equation*}
    p(f_*|X_*,w) \propto exp\left(-\frac{1}{2}\frac{1}{\sigma^2_n}(f_* - W^TX_*)^2\right)
\end{equation*}

Multiplying $P(f_*|X_*,W)$ and $p(W|X,y)$ to get our conditional $p(f_*, W|X_*,X,y)$, and add the exponents: 
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(\frac{1}{2}(-W^TAW + 2b^TW) + \left(-\frac{1}{2}\frac{1}{\sigma^2_n}(f_* - W^TX_*)^2\right)\right)
\end{equation*}

Combining the terms inside the exponent:
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^TAW - 2b^TW + \frac{1}{\sigma^2_n}(f_* - W^TX_*)^2\right)\right)
\end{equation*}

Expanding the squared term:
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^TAW - 2b^TW + \frac{1}{\sigma^2_n}(f_*^2 - 2f_*W^TX_* + W^TX_*X_*^TX_*)\right)\right)
\end{equation*}

Similar to our posterior, we can rearrange this to be a quadratic, linear and constant term in $W$:
\begin{equation} \label{eq:predictive_completing_square}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^T\left(A + \frac{1}{\sigma^2_n}X_*X_*^T\right)W - 2\left(b + \frac{1}{\sigma^2_n}f_*X_*\right)^TW + \frac{1}{\sigma_n^2}f_*^2\right)\right)
\end{equation}

We can define new terms $A_*$ and $b_*$ to simplify this expression:
\begin{equation*}
    \begin{aligned}
        A_* = A + \frac{1}{\sigma^2_n}X_*X_*^T \\
        b_* = b + \frac{1}{\sigma^2_n}f_*X_*
    \end{aligned}
\end{equation*}

Substituting into \ref{eq:predictive_completing_square}:
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^TA_*W - 2b_*^TW + \frac{1}{\sigma_n^2}f_*^2\right)\right)
\end{equation*}

Integrating out $W$ to get our predictive distribution:
\begin{equation} \label{eq:predictive_distribution_1}
    p(f_*|X_*,X,y) = \int p(f_*,W|X_*,X,y)dW \propto \int exp\left(-\frac{1}{2}\left(W^TA_*W - 2b_*^TW + \frac{1}{\sigma_n^2}f_*^2\right)\right)dW
\end{equation}

Factoring out $\frac{1}{\sigma_n^2}f_*^2$ as it does not depend on $W$ (since $\int exp(X) dX = exp(X)$):
\begin{equation*}
    = exp\left(-\frac{1}{2}\frac{1}{\sigma_n^2}f_*^2\right) \times \int exp\left(-\frac{1}{2}\left(W^TA_*W - 2b_*^TW\right)\right) dW
\end{equation*}

Evaluating the RHS multivariate Gaussian integral:
\begin{equation*}
    \int exp\left(-\frac{1}{2} \left( W^TA_*W - 2b_*^TW \right) \right) dW = \frac{(2\pi)^{D/2}} {\sqrt{|A_*|}} exp\left( \frac{1}{2} b_*^TA_*^{-1}b_* \right)
\end{equation*}

Substituting back into \ref{eq:predictive_distribution_1}: 
\begin{equation*}
    p(f_*|X_*,X,y) \propto exp\left(-\frac{1}{2}\frac{1}{\sigma_n^2}f_*^2\right) + \frac{(2\pi)^{D/2}}{\sqrt{|A_*|}} \cdot exp\left(\frac{1}{2}b_*^TA_*^{-1}b_*\right)  
\end{equation*}
    
Now that no part of our expression is dependent on W, we need an expression for everything that depends on $f_*$. 

Absorbing the second term (since it does not depend on $f_*$) into the proportionality constant, and combining the remaining exponential terms by adding their powers:
\begin{equation*}
    p(f_*|X_*,X,y) \propto \exp\left(-\frac{1}{2}\frac{1}{\sigma_n^2}f_*^2 + \frac{1}{2}b_*^TA_*^{-1}b_*\right)
\end{equation*}

Similar to deriving properties from our posterior, we can rearrange this expression and complete the square to derive the properties of our predictive distribution:
\begin{equation} \label{eq:predictive_gaussian}
    p(f_*|X_*,W) \sim N(X_*^TA^{-1}b, X_*^TA^{-1}X_*)
\end{equation}
The variance is quadratic in $X_*$ with $A^{-1}$, showing that predictive uncertainties grow with size of $X_*$.

\subsubsection{Projections of inputs into feature space}
One problem with this model is that it assumes a linear relationship between $X$ and $y$. We can project our inputs into a higher dimensional feature space and apply a linear model in this space to express non-linear relationships between $X$ and $y$. 

Defining $\phi(X)$ as a function that maps a $D$-dimensional input vector $X$ into an $N$ dimensional feature space, our standard linear model becomes:
\begin{equation*}
    f(X) = \phi(X)^T W
\end{equation*}
For example, a scalar $x$ could be projected into the space of powers of $x$: $\phi(x) = [1, x, x^2, \ldots, x^d]^T$ for a polynomial basis expansion of degree $d$ to represent a $d$-power relationship between $x$ and $y$.
.
%   \item $\phi(x)$ must be independent of $W$ so that we can learn $W$ from the data
Substituting $\phi(X)$ for $X$ in \ref{eq:predictive_gaussian}:
\begin{equation} \label{eq:predictive_gaussian_phi}
    p(f_*|X_*,X,y) = N(\phi(X_*)^TA_{\phi}^{-1}b_{\phi} , \phi(X_*)^TA_{\phi}^{-1}\phi(X_*))
\end{equation}

Where $A_{\phi}$ and $b_{\phi}$ are now:
\begin{equation*}
    A_{\phi} = \Sigma_p^{-1} + \frac{1}{\sigma^2_n}\phi(X)^T\phi(X)
    b_{\phi} = \frac{1}{\sigma^2_n}\phi(X)^Ty
\end{equation*}

\subsubsection{Computational issues}
\paragraph{Avoiding inversion of $A_{\phi}$}
\ref{eq:predictive_gaussian_phi} requires inverting the $N \times N$ matrix $A_{\phi}$, where $N$ is dimension of feature space, to get the expected value and variance. 

Typically, matrices are inverted using Gaussian elimination. We need to perform a "forward" pass which requires $N$ pivots on every row and column, $N$ eliminations per pivot, and up to $2N$ columns to update, resulting in an $O(N^3)$ time complexity. Then, we need to perform a backwards pass in the opposite direction which is another $O(N^3)$ operation. Finally, we need to multiply the inverse by the RHS vector $b_{\phi}$, which is an $O(N^2)$ operation but appears trivial next to these two cubic steps. 

We can mitigate this for a particular class of high-dimensional $N > n$ problems by restating the predictive distribution in terms of the number of training data points $n$ which would require inverting an $n \times n$ matrix instead. For polynomial basis expansions, $N$ is degree $D$ multiplied by number of features, so $N$ can be very large or even infinite (e.g. SE). % Some data domains (e.g. text classification, genomic data) have very high dimensional feature spaces.

Substituting $b_{\phi}$ into our predictive distribution mean:
\begin{equation*}
    \mathbb{E}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*)^T \cdot A_\phi^{-1} \cdot \left[\frac{1}{\sigma_n^2}\phi(X)^Ty\right]
\end{equation*}

Rearranging to isolate $A_\phi^{-1}\phi(X)$
\begin{equation*}
    = \frac{1}{\sigma_n^2}\left[A_{\phi}^{-1}\phi(X)\right]^Ty
\end{equation*}

We can use the Sherman-Morrison-Woodbury identity (SMW) to get an expression for $A_{\phi}^{-1}$ directly, where $K =\phi(X)^T\Sigma_p\phi(X)$:
\begin{equation*}
    A_{\phi}^{-1} = \Sigma_p - \Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p
\end{equation*}

For the mean, we can use the SMW identity again to get an expression for $A_{\phi}^{-1}\phi(X)$
\begin{equation} \label{eq:A_phi_inverse_phi_X}
    A_{\phi}^{-1}\phi(X) = \sigma_n^2\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}
\end{equation}

Substitute in \ref{eq:A_phi_inverse_phi_X} into our \ref{eq:predictive_gaussian_phi}:
\begin{equation*}
    \mathbb{E}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*) \frac{1}{\sigma_n^2}\left[\sigma_n^2\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\right]^Ty
\end{equation*}

$\frac{1}{\sigma_n^2}$ and $\sigma_n^2$ cancel out, leaving us with this final expression for the mean:
\begin{equation} \label{eq:alt_predictive_mean_phi}
    \mathbb{E}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*)^T \cdot \Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}y
\end{equation}

For the variance, we cannot use the Sherman-Morrison identity to arrive at an expression for $A_{\phi}^{-1}\phi(X_*)$ because $\phi(X_*)$ is an arbitrary N-vector, not one of the columns of $\phi(X)$. Instead, we use the $A_{\phi}^{-1}$ expression we derived earlier to get an expression for $A_{\phi}^{-1}\phi(X_*)$:
\begin{equation*}
    A_{\phi}^{-1}\phi(X_*) = \Sigma_p \cdot \phi(X_*) - \Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p \cdot \phi(X_*)
\end{equation*}

 Substituting this into \ref{eq:predictive_gaussian_phi}:
\begin{equation} \label{eq:alt_predictive_variance_phi}
    \text{Var}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*)^T\Sigma_p\phi(X_*) - \phi(X_*)^T\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p\phi(X_*)
\end{equation}

With our alternative mean \ref{eq:alt_predictive_mean_phi} and variance \ref{eq:alt_predictive_variance_phi}, we can form an alternative expression for our predictive distribution:
\begin{equation} \label{eq:alt_predictive_gaussian_phi}
    \begin{aligned}
        p(f_*|X_*,X,y) = \mathcal{N}( \\
        \phi(X_*)^T\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}y , \\
        \phi(X_*)^T\Sigma_p\phi(X_*) - \phi(X_*)^T\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p\phi(X_*) \\
        )
    \end{aligned}
\end{equation}

With this alternative formulation, we need to invert the $n \times n$ matrix $K + \sigma_n^2I$ only. Geometrically, $n$ datapoints can span at most $n$ dimensions in the feature space - if $N > n$, the data forms a subspace of the feature space.

\paragraph{Kernels and the kernel trick}
In \ref{eq:alt_predictive_gaussian_phi}, $\phi(.)$ is always an inner product of a positive definite correlation matrix $\Sigma_p$, but with different arrangements of $\phi(X)$ and $\phi(X_*)$. We can define $k(X,X') = \phi(X)^T\Sigma_p\phi(X')$ as a covariance function or kernel, where $X$ and $X'$ are either $X$ or $X_*$. For example, in \ref{eq:alt_predictive_gaussian_phi} the definition of $K = \phi(X)^T\Sigma_p\phi(X)$ becomes $K = k(X,X)$.

Introducing $\psi(X)$ to better represent $k(X,X')$ as an inner product:
\begin{equation*}
    \begin{aligned}
        \psi(X) = \phi(X)\Sigma_p^{1/2} \\
        k(X,X') = \psi(X)^T\psi(X') \\
    \end{aligned}
\end{equation*}
These inner product representations require us to compute $\phi(X)$ and $\phi(X')$ in the feature space. A higher-dimensional feature space requires more compute to evaluate $\phi(X)$ and more memory to store $\phi(X)$ and $\phi(X')$.

Instead, the representer theorem guarantees that we can find an equivelant kernel that does not require us to explicitly compute $\phi(X)$ or $\phi(X')$ in the feature space. With this "kernel trick" we avoid the associated memory and computational costs of explicitly computing $\phi(X)$ and $\phi(X')$. Since computing the kernel directly is more convenient than the feature vectors themselves, these kernels become the object of primary interest.

% Good detailed material on these non-GP approaches. is this material going to relate in some way to the GP approach and part of the project? For example using some of it later, or discussing similarities and differences. Will be good if this is the case somehow, otherwise it may seem excessive, considering the project's topic. (I have not seen the other 3 pages yet)

For example, if we had some polynomial transformation $\phi(X) = [1, x^1, ..., x^D]^T$ and $\Sigma_p$ as an identity matrix, we could define $k(X,X')$ as inner products:
\begin{equation*}
    \begin{aligned}
        \psi(X) = [1, x^1, ..., x^D]^T
        k(X,X') = \psi(X)^T\psi(X')
    \end{aligned}
\end{equation*}
This approach requires arranging $\phi$ and $\phi(X')$ into a $D$ sized vector, then taking the dot product. This is trivial for small $D$, but as $D$ becomes infinite (e.g. RBF kernel), arranging a $D$ sized vector requires too much memory and the dot product becomes computationally expensive.

Instead, we can define $k(X,X')$ as an equivelant function of $X$ and $X'$ directly:
\begin{equation*}
    k(X,X') = (1 + X \cdot X')^D
\end{equation*}
This is the polynomial kernel, which is equivalent to our original polynomial basis expansion $\phi(X)$ without explicitly computing $\phi(X)$.



\subsection{Function-space view}

\subsubsection{Gaussian processes (GP)}

\paragraph{Bayesian linear model}
We can define our Bayesian linear model of a real process $f(X)$ entirely in terms of mean function $m(X)$ and covariance function $k(X,X')$:
\begin{equation} \label{eq:gp_bayesian}
    \begin{aligned}
        m(X) = \phi(X)^T\mathbb{E}[W] = \phi(X)^T[0] = 0 \\
        k(X,X') = \phi(X)^T\mathbb{E}[WW^T]\phi(X') = \phi(X)^T\Sigma_P\phi(X')
    \end{aligned}
\end{equation}

Our covariance function here is in inner product form. The kernel trick here uses the squared exponential (SE) covariance function, also known as the radial basis function (RBF) or Gaussian kernel:
\begin{equation*}
    k(f(X), f(X')) = \exp\left(-\frac{1}{2}\frac{|X - X'|^2}{l^2}\right)
\end{equation*}
It can be shown that SE corresponds to a Bayesian linear regression model with infinite basis functions. 

% For SE, covariance is almost unity between outputs whose inputs are close together, but decays exponentially as inputs get further apart.   The Mercer theorem states that for every positive definite covariance function $k(X,X')$, there exists a possibly infinite set of basis functions. SE can also be obtained from the linear combination of infinite Gaussian-shaped basis functions. Because SE is infinitely differentiable, it produces smooth functions.


\paragraph{Function evaluations to a random function}
We can choose a subset $X_{*1}$ from our test data $X_*$ and apply it to our model get some function evaluations $f(X_{*1})$. $f(X_{*1})$ can be described as a multivariate Gaussian distribution, e.g. in the Bayesian linear model $f(X_{*1}) \sim N(0, k(X_{*1}, X_{*1}))$. Each output $f(X_{\theta*1})$ in our $f(X_{*1})$ vector is a random variable with mean $0$ and covariance with each other $K_{\theta\theta'} = k(X_{\theta*}, X_{\theta'*})$. There exists some random function $g(X_{*1})$ for our subsets such that $f(X_{*1}) = g(X_{*1})$. We only know the value of $g(X_{*1})$ at the points $X_{*1}$, so $g(X_{*1}) = {X_{*1} : f(X_{*1})}$. Because $g(X)$ entirely consists of random points, we can think of $g(X_{*1})$ as a random function and our distribution $f(X)$ can be seen as a distribution of these random $g(X)$ functions. We can recover our individual $g(X_{*1}$ thanks to consistency - if we marginalised out our subset from the entire distribution $f(X_*)$, we would recover the subset distribution $N(0, K_*(X_{*1}, X_{*1}))$ that describes our random function $g(X_{*1})$.
% Michael: Ok, I see that do try to relate to the earlier material, which is good.

\paragraph{Definition of a GP}
A GP is a collection of random variables, any finite number of which have a joint Gaussian distribution. Ultimately, GPs describe a distribution of random functions where each drawn function is a $g(X)$ sample from the GP.
\begin{equation*}
    \begin{aligned}
        f(X) \sim \mathcal{GP}(m(X), k(X,X')), \\
        m(X) = \mathbb{E}[f(X)], \\
        k(X,X') = \text{Cov}(f(X), f(X')) = \mathbb{E}[(f(X) - m(X))(f(X') - m(X'))]
    \end{aligned}
\end{equation*}
% These random variables represent the value of $f(X)$ at location $X$. often Gaussian processes are defined over time so $X$ can be a time point. The covariance function specifies the covariance between pairs of random variables.

\paragraph{Consistency requirement}
This definition implies a consistency requirement - any group of functions drawn from our GP can be described by the same distribution as our GP. For example, if our GP implies that $(f(X_1), f(X_2)) \sim \mathcal{N}(\mu, \Sigma)$, then $(f(X_1) \sim \mathcal{N}(\mu_1, \Sigma) and f(X_2) \sim \mathcal{N}(\mu_2, \Sigma))$ where $\mu_{\theta} = m(X_{\theta})$ and $\Sigma_{\theta\theta} = k(X_{\theta}, X_{\theta})$. This requirement is also called the marginalisation property, because to get the smaller distribution of $f(X_1)$ we marginalise out the larger distribution of $f(X_1), f(X_2)$ by integrating the larger distribution wrt $f(X_2)$. Consistency is automatically gained if our covariance function specifies entries in a covariance matrix.
    
\subsubsection{Predictive distributions with noise-free observations}
\paragraph{Prior distribution over functions}
$f(X)$ and $f(X_*)$ are jointly distributed according to the prior:
\begin{equation} \label{eq:joint_prior}
    \begin{pmatrix}
        f(X) \\ f(X_*)
    \end{pmatrix} \sim N\left(
    \begin{pmatrix}
        0 \\ 0
    \end{pmatrix},
    \begin{pmatrix}
        K(X,X) & K(X,X_*) \\ 
        K(X_*,X) & K(X_*,X_*)
    \end{pmatrix}
    \right)
\end{equation}

\paragraph{Posterior distribution of functions}
To get the posterior distribution of functions given the training data and our prior, we can condition the joint prior distribution on the training data. Intuitively, this is like generating random functions $g(X)$ and rejecting those that do not pass through the training data. Probabilistically, we condition our joint Gaussian prior distribution on the observations to produce a predictive distribution for some point we want to predict for $X_*$. 

Substituting our prior and our conditioning $X$ into the Gaussian multivariate conditioning identity:
\begin{equation*}
    \begin{aligned}
        p(f(X_*)|X_*, X, f(X)) \sim N( \\
        [0] + [K(X_*,X)][K(X,X)]^{-1}([f(X)] - [0]), \\
        [K(X*,X*)] - [K(X_*,X)][K(X,X)]^{-1}[K(X,X_*)] \\
        )
    \end{aligned}
\end{equation*}

Although we condition on $X_*$, $X$, and $f(X)$, we only substitute $f(X)$ because $X_*$ and $X$ are known constants, but $f(X)$ is random because it is a sample from the prior. We also swap $f(X_*)$ and $f(X)$ in our prior to match the conditioning identity, such that our input vector into the conditioning identity is $(f(X_*), f(X))^T$.
    
Simplifying the last term in the mean:
\begin{equation} \label{eq:conditioning}
    \begin{aligned}
        p(f(X_*)|X_*, X, f(X)) \sim N( \\
        K(X,X_*)K(X,X)^{-1}f(X), \\
        K(X_*,X_*) - K(X_*,X)K(X,X)^{-1}K(X,X_*) \\
        )
    \end{aligned}
\end{equation}


\subsubsection{Predictive distributions with noisy observations}
\paragraph{Noisy observations prior}
It is typical to not have the noise-free function evaluations $f(X)$ as our training data, but instead our noisy observations $y$. We can simply add $\epsilon$:
\begin{equation*}
    \text{Cov}(y_p, y_q) = K(X_p, X_q) + \sigma^2_n\delta_{pq}
\end{equation*}
$\delta_{pq}$ represents our independence condition in 1D. This is the Kronecker delta, which returns 1 if indices ($p$, $q$) are equal and 0 otherwise. $\sigma^2_n$ is the noise variance, which is a constant for all observations.
    
In matrix form:
\begin{equation*}
    \text{Cov}(Y) = k(X,X) + \sigma^2_nI
\end{equation*}
    
This gives us this prior:
\begin{equation} \label{eq:joint_prior_noisy}
    \begin{pmatrix}
        Y \\ f(X_*)
    \end{pmatrix} \sim N\left(
    \begin{pmatrix}
        0 \\ 0
    \end{pmatrix},
    \begin{pmatrix}
        K(X,X) + \sigma^2_nI & K(X,X_*) \\ K(X_*,X) & K(X_*,X_*)
    \end{pmatrix}
    \right)
\end{equation}

\paragraph{Noisy observations posterior}
As before, we can form a predictive distribution using the Gaussian multivariate conditioning identity:
\begin{equation} \label{eq:predictive_distribution_noisy}
    \begin{aligned}
        p(f(X_*)|X_*, X, Y) \sim N( \\
        K(X_*,X)[K(X,X) + \sigma^2_nI]^{-1}Y, \\
        K(X_*,X_*) - K(X_*,X)[K(X,X) + \sigma^2_nI]^{-1}K(X,X_*) \\
        )
    \end{aligned}
\end{equation}

Substituting $k(X,X') = \phi(X)^T\Sigma_p\phi(X')$ into here gives us the exact same result as \ref{eq:alt_predictive_gaussian_phi}.

Our variance is independent of the targets $y$ and only depends on our inputs $X$ and $X_*$. Our variance is two terms: our prior covariance $K(X_*,X_*)$, a term representing the information the observations give us about the function. As before, we can compute the predictive distribution of $y_*$ by adding the noise term $\sigma^2_nI$ to the variance.

\subsubsection{Marginal likelihood}
Even though we are working within the Bayesian paradigm, certain practical subroutines e.g. kernel hyperparameter optimisation algorithms need a likelihood to maximise. The marginal likelihood $p(Y|X)$ is a measure of how well our GP fits the data:
\begin{equation} \label{eq:marginal_likelihood}
    p(Y|X) = \int p(Y|f,X) p(f|X) df
\end{equation}
$p(f|X)$ is our familiar prior distribution over weights $\sim N(0, K)$ which we use here to represent the complexity of $f$. $p(y|f,X)$ is the likelihood of our observations given $p(y|f,X) \sim N(f, \sigma^2_nI)$, and represents how well $f$ maps $X$ to $y$. 

Our weight's mean will always be the same under our prior, but our $K(X,X')$ tells us how "wiggly" our function is. The closer our $p(f|X)$ distribution is to the true complexity of the function, the higher our marginal likelihood. For example, for SE if our data is close together, then $|X - X'|$ becomes small and our covariance $k(X,X')$ on our function distribution prior is large. Therefore, we get a high variety of functions and a higher probability of sampling a more complex function. % Conversly,ew data $k(X_*, X_*')$ that is far away from each other and would require a more complex function, its density in this prior smooth distribution will be low, so we get a low $p(f|X_*)$. 
    
We can express $p(Y|X)$ as a Gaussian integral over the joint distribution of $f$ and $Y$ ($p(Y,f|X)$), and marginalise out $f$ to get this PDF:
\begin{equation} \label{eq:log_marginal_likelihood}
    log p(Y|X) = -\frac{1}{2}Y^T(K(X,X) + \sigma^2_nI)^{-1}Y - \frac{1}{2}log|K + \sigma^2_nI| - \frac{n}{2}log(2\pi)
\end{equation}

Alternatively, from \ref{eq:linear_model} we know that $y$ is Gaussian. Since both $y$ and $f$ are Gaussian, we can simply add their means and variances: 
\begin{equation*}
    p(Y|X) = N(0, K(X, X) + \sigma^2_nI)
\end{equation*}
We can plug these mean and variances into the Gaussian PDF \ref{eq:gaussian_pdf} to get \ref{eq:log_marginal_likelihood}. Assembling the likelihood is necessary for training to select hyperparameters, but the $K(X, X) + \sigma^2_nI$ inversion costs $O(n^3)$ to compute. 


\subsubsection{Algorithm for Gaussian processes}
Here is an algorithm for computing the predictive distribution for predictions and log-likelihood for training that uses Cholesky decomposition \ref{eq:cholesky-factor} to address the matrix inversion requirement.
\begin{enumerate}
    \item Take in inputs $X$, outputs $y$, covariance function $k$, noise level $\sigma^2_n$, and test input $X_*$
    \item $L = \text{cholesky}(K(X,X) + \sigma_n^2I)$
    \begin{itemize}
        \item Compute the Cholesky factor \ref{eq:cholesky-factor} for the $[K(X,X) + \sigma^2_nI]$ matrix that needs to be inverted for training and inference
    \end{itemize}
    \item $alpha = L^T \backslash (L \backslash y)$
    \begin{itemize}
        \item Find $[K(X,X) + \sigma^2_nI]^{-1}y$ using Cholesky decomposition \ref{eq:cholesky-factor}, which is equivelant to solving the linear system $L^T L \cdot alpha = y$.
    \end{itemize}
    \item $mu = K(X_*, X)^T \cdot alpha$
    \begin{itemize}
        \item Compute the mean
    \end{itemize}
    \item $v = L \backslash K(X_*, X)^T$
    \begin{itemize}
        \item Compute $v = L^{-1} K(X_*, X)$ 
    \end{itemize}
    \item $var = K(X_*, X_*) - v^T v$
    \begin{itemize}
        \item Plug $v$ into the variance restated in terms of the Cholesky factors, where $v^T v = (L^{-1} K(X_*, X))^T (L^{-1} K(X_*, X)) = K(X_*, X)^T [K(X,X) + \sigma^2_nI]^{-1} K(X_*, X)$ 
    \end{itemize}
    \item $\text{log} p(Y|X) = -\frac{1}{2}y^T \cdot alpha - \frac{1}{2}log|K(X,X) + \sigma^2_nI| - \frac{n}{2}log(2\pi)$
    \begin{itemize}
        \item Compute the log marginal likelihood 
    \end{itemize}
    \item Return the mean $mu$, variance $var$, and log marginal likelihood
\end{enumerate}
$\backslash$ denotes a linear system solver, i.e. $L \backslash y$ solves the linear system $Lx = y$ for $x$. 

% \subsection{Smoothing and equivelant kernels \cite{gp-ml}}
% 
% \subsubsection{Linear predictors and smoothers}
% The representer theorem states that we can represent a kernel in finite-dimensional space, even if properties of our method mean we have an infinite-dimensional feature space. For example, using SE we could obtain an infinite number of basis functions, so our data could be projected into an infinite dimensional space. Any other function than the mean drawn from our predictive distribution would have to be in infinite-dimension space. However, our mean is dependent on our $n$-length training labels $y$. Therefore, 

% The mean of our predictive distribution can be represented as a "linear predictor", or as a linear combination of $X$ and our covariance function:
% \begin{equation} \label{eq:gp_linear_predictor}
%     \begin{aligned}
%         \alpha_i = K(X_i, X_i) + \sigma_n^2I)^{-1}f(X)_i \\
%         \mathbb{E}_{p(f(X_*)|X_*,X,Y)}[f(X_*)] = \sum_{i=1}^n \alpha_i k(X_*, X_i)
%     \end{aligned}
% \end{equation}
% Here, $\alpha_i$ is a "linear smoother" because it is a linear combination of $y$ and our covariance function.

% Understanding how our predictive distribution's mean varies with its inputs is difficult because of the first term $[(K(X, X) + \sigma_n^2I)]^-1$: it is dependent on exact values of $X$, and it requires the inversion of $K(X, X) + \sigma^2_nI$. Instead, we can reformulate $E_{p(f(X_*)|X_*,X,Y)}[f(X_*)]$ as an "equivelant kernel" to remove the dependence on $X$. 
% which uses a kernel smoother (also known as the Nadaraya-Watson estimator)  
% 
% Firstly, we rewrite our mean such that our mean function is a "linear smoother", or a function of the covariance function and our training data labels $Y$:
% \begin{equation} \label{eq:gp_linear_smoother}
%     \begin{aligned}
%         H(X_*) = [K(X, X) + \sigma^2_nI]^{-1}K(X_*,X) \\
%         \mathbb{E}_{y_*|X_*,X,y)}[f(X_*)] = H(X_*)^T Y
%     \end{aligned}
% \end{equation}
% This vector of functions $H(X_*)$ is called a weight function and contains our problematic term. 
% 
% Instead of inputting the values of $x_i$ directly into the kernel function, we can use $x_i$ centred around $X_*$: 
% \begin{equation*}
%     k_i = k(|x_i - x_{*i}|/l)
% \end{equation*}
% Where $l$ is some length scale hyperparameter. 
% 
% explain what we gain from this
% 
% Thus, our predictive distribution mean becomes:
% \begin{equation*}
%     \mathbb{E}_{p(f(X_*)|X_*,X,Y)}[f(X_*)] = \sum_{i=1}^n w_i y_i
% \end{equation*}
% where $w_i = k_i / \sum_{j=1}^n k_j$ is a


% \subsubsection{Explicit basis functions}
