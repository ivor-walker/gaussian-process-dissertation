\section*{Abstract}
Gaussian processes (GPs) provide a flexible, non-parametric Bayesian framework for statistical modelling and machine learning, enabling principled uncertainty quantification in regression, classification, and beyond. This dissertation undertakes a systematic investigation of GP models, beginning with a conceptual bridge between the weight-space view of Bayesian linear models and the function-space formulation of GPs. 

Using both simulated and real datasets, we compare a range of widely used covariance (correlation) functions, assessing their implications for smoothness, differentiability, and spectral properties. A unified notion of smoothness is developed, linking length-scale and mean-square differentiability through Taylor expansion, upcrossing theory, and spectral decay rates.

Alongside theoretical exploration, the work addresses key practical challenges in GP modelling, notably the cubic computational cost of large-scale inference and identifiability issues in residual modelling after linear fits. Two distinct computational strategies dominate: stochastic variational Gaussian processes (SVGP), which provide slower and less accurate approximations for any kernel, and structured kernel methods such as celerite, which exploit algebraic properties of specific kernels highly accurate and fast inversion. 
Benchmark experiments on galaxy spectral energy distribution residuals highlight a trade-off between identifiability and computational efficiency: the squared exponential kernel performs well but is computationally intractable for large datasets, while celerite is efficient but performed poorly. The findings underscore that the choice of covariance function and computational strategy must be guided jointly by the data's underlying structure and the end goal. Recommendations are provided for future work, including GPU-accelerated SVGP, modified Matern kernels to emulate squared exponentials, and other structured, low-dimensional GP approximations. 

% This project concerns the Gaussian process. The student will be required to understand the Gaussian process model and the different applications where it is employed, in relation to statistical modelling and machine learning, by reading relevant textbooks and publications. Then, using simulated and real data, different correlation functions will be explored and compared. Other aspects of the Gaussian process (GP) will be explored. Computational efficiency, for instance, as using the GP typically requires the inversion of large matrices, or identifiability issues when the GP is used to model the residuals after a linear model is fitted.
