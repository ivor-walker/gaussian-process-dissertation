@book{gp-ml,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    title = {Gaussian Processes for Machine Learning},
    publisher = {The MIT Press},
    year = {2005},
    month = {11},
    isbn = {9780262256834},
    doi = {10.7551/mitpress/3206.001.0001},
    url = {https://doi.org/10.7551/mitpress/3206.001.0001},
    eprint = {https://direct.mit.edu/book-pdf/2514321/book\_9780262256834.pdf},
}

@book{gp-dynamic-systems,
	title     = "Modelling and control of dynamic systems using Gaussian process models",
	author    = "Kocijan, J",
	publisher = "Springer International Publishing",
	series    = "Advances in industrial control",
	edition   =  1,
	month     =  nov,
	year      =  2015,
	address   = "Cham, Switzerland",
	copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
	language  = "en"
}

@ARTICLE{big-data,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  keywords={Kernel;Scalability;Sparse representation;Complexity theory;Computational modeling;Ground penetrating radar;Predictive models;Big data;Gaussian process regression (GPR);local approximations;scalability;sparse approximations},
  doi={10.1109/TNNLS.2019.2957109}
}

@article{materials,
  title={Gaussian process regression for materials and molecules},
  author={Deringer, Volker L and Bart{\'o}k, Albert P and Bernstein, Noam and Wilkins, David M and Ceriotti, Michele and Cs{\'a}nyi, G{\'a}bor},
  journal={Chemical Reviews},
  volume={121},
  number={16},
  pages={10073--10141},
  year={2021},
  publisher={ACS Publications}
}

@article{cosmography,
  title = {Gaussian process cosmography},
  author = {Shafieloo, Arman and Kim, Alex G. and Linder, Eric V.},
  journal = {Phys. Rev. D},
  volume = {85},
  issue = {12},
  pages = {123530},
  numpages = {9},
  year = {2012},
  month = {6},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.85.123530},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.85.123530}
}

@article{vgp,
  title={The variational Gaussian process},
  author={Tran, Dustin and Ranganath, Rajesh and Blei, David M},
  journal={arXiv preprint arXiv:1511.06499},
  year={2015}
}

@article{gprn,
  title={Gaussian process regression networks},
  author={Wilson, Andrew Gordon and Knowles, David A and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1110.4411},
  year={2011}
}

@article{emulators,
  title={Diagnostics for Gaussian process emulators},
  author={Bastos, Leonardo S and O’hagan, Anthony},
  journal={Technometrics},
  volume={51},
  number={4},
  pages={425--438},
  year={2009},
  publisher={Taylor \& Francis}
}

@inproceedings{freeform-kernels,
 author = {Bonilla, Edwin V and Chai, Kian and Williams, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-task Gaussian Process Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
 volume = {20},
 year = {2007}
}


@InProceedings{deriving-kernels,
  title = 	 {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  author = 	 {Wilson, Andrew and Adams, Ryan},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1067--1075},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {6},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wilson13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wilson13.html},
  abstract = 	 {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}

@inproceedings{hierarchical-kernels,
 author = {Schwaighofer, Anton and Tresp, Volker and Yu, Kai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Learning Gaussian Process Kernels via Hierarchical Bayes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume = {17},
 year = {2004}
}

@inproceedings{multi-kernels,
  title={Multi-kernel Gaussian processes},
  author={Melkumyan, Arman and Ramos, Fabio},
  booktitle={IJCAI Proceedings-International Joint Conference on Artificial Intelligence},
  volume={22},
  number={1},
  pages={1408},
  year={2011}
}

@article{learning-kernels,
  title={Learning “best” kernels from data in Gaussian process regression. With application to aerodynamics},
  author={Akian, J-L and Bonnet, Luc and Owhadi, Houman and Savin, {\'E}ric},
  journal={Journal of Computational Physics},
  volume={470},
  pages={111595},
  year={2022},
  publisher={Elsevier}
}

@Manual{gaupro,
    title = {GauPro: Gaussian Process Fitting},
    author = {Collin Erickson},
    year = {2025},
    note = {R package version 0.2.15.9000, commit 6c7a7a8055506229b6d02650aca9664a84f640f8},
    url = {https://github.com/CollinErickson/GauPro},
}

@ARTICLE{signals-processing,
  author={Perez-Cruz, Fernando and Van Vaerenbergh, Steven and Murillo-Fuentes, Juan Jose and Lazaro-Gredilla, Miguel and Santamaria, Ignacio},
  journal={IEEE Signal Processing Magazine}, 
  title={Gaussian Processes for Nonlinear Signal Processing: An Overview of Recent Advances}, 
  year={2013},
  volume={30},
  number={4},
  pages={40-50},
  abstract={Gaussian processes (GPs) are versatile tools that have been successfully employed to solve nonlinear estimation problems in machine learning but are rarely used in signal processing. In this tutorial, we present GPs for regression as a natural nonlinear extension to optimal Wiener filtering. After establishing their basic formulation, we discuss several important aspects and extensions, including recursive and adaptive algorithms for dealing with nonstationarity, low-complexity solutions, non-Gaussian noise models, and classification scenarios. Furthermore, we provide a selection of relevant applications to wireless digital communications.},
  keywords={Machine learning;Learning systems;Gaussian processes;Nonlinear estimation;Noise measurement;Wiener filters;Adaptive algorithms},
  doi={10.1109/MSP.2013.2250352},
  ISSN={1558-0792},
  month={7},
}

@article{choosing-kernels,
author = {Akian, Jean-Luc and Bonnet, Luc and Owhadi, Houman and Savin, Eric},
year = {2022},
month = {08},
pages = {111595},
title = {Learning “best” kernels from data in Gaussian process regression. With application to aerodynamics},
volume = {470},
journal = {Journal of Computational Physics},
doi = {10.1016/j.jcp.2022.111595}
}

@misc{additive-kernels,
      title={Additive Covariance Kernels for High-Dimensional Gaussian Process Modeling}, 
      author={Nicolas Durrande and David Ginsbourger and Olivier Roustant and Laurent Carraro},
      year={2011},
      eprint={1111.6233},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1111.6233}, 
}


@InProceedings{random-subsampling,
  title = 	 {On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis},
  author =       {Hayashi, Kohei and Imaizumi, Masaaki and Yoshida, Yuichi},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2055--2065},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/hayashi20a/hayashi20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/hayashi20a.html},
  abstract = 	 {In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of training data, we show provable guarantees on the accuracy of the predictive mean/variance and its generalization ability.For analysis, we consider embedding kernel matrices into graphons, which encapsulate the difference of the sample size and enables us to evaluate the approximation and generalization errors in a unified manner. The experimental results show that the subsampling approximation achieves a better trade-off regarding accuracy and runtime than the ystrom and random Fourier expansion methods.}
}

@article{sparse-kernels,
author = {Gneiting, Tilmann},
year = {2002},
month = {11},
pages = {493-508},
title = {Compactly Supported Correlation Functions},
volume = {83},
journal = {Journal of Multivariate Analysis},
doi = {10.1006/jmva.2001.2056}
}

@BOOK{bochner,
  title     = "The theory of stochastic processes {I}",
  author    = "Gikhman, Iosif I and Skorokhod, Anatoli V",
  publisher = "Springer",
  series    = "Classics in mathematics",
  month     =  mar,
  year      =  2004,
  address   = "Berlin, Germany",
  language  = "en"
}

@article{fourier-moments,
 title= {Multivariate integration and approximation for random fields satisfying Sacks-Ylvisaker conditions}, 
 volume={5}, 
 DOI={10.1214/aoap/1177004776}, 
 number={2}, 
 journal={The Annals of Applied Probability}, 
 author={Ritter, Klaus and Wasilkowski, Grzegorz W. and Wozniakowski, Henryk}, 
 year={1995}, 
 month={May}
} 

@BOOK{nystrom,
  title     = "The numerical treatment of integral equations",
  author    = "Baker, Christopher T H",
  publisher = "Oxford University Press",
  series    = "Monographs on Numerical Analysis",
  month     =  jan,
  year      =  1978,
  address   = "London, England",
  language  = "en",
  pages = "67, 67"
}

@article{sor,
author = {Joukov, Vladimir and Kuli\'{c}, Dana},
title = {Fast Approximate Multioutput Gaussian Processes},
year = {2022},
issue_date = {July-Aug. 2022},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {37},
number = {4},
issn = {1541-1672},
url = {https://doi.org/10.1109/MIS.2022.3169036},
doi = {10.1109/MIS.2022.3169036},
abstract = {Gaussian processes regression models are an appealing machine learning method as they learn expressive nonlinear models from exemplar data with minimal parameter tuning and estimate both the mean and covariance of unseen points. However, cubic computational complexity growth with the number of samples has been a long standing challenge. Training requires the inversion of $N times N$N\texttimes{}N kernel at every iteration, whereas regression needs computation of an $m times N$m\texttimes{}N kernel, where $N$N and $m$m are the number of training and test points, respectively. This work demonstrates how approximating the covariance kernel using eigenvalues and functions leads to an approximate Gaussian process with significant reduction in training and regression complexity. Training now requires computing only an $N times n$N\texttimes{}n eigenfunction matrix and an $n times n$n\texttimes{}n inverse, where $n$n is a selected number of eigenvalues. Furthermore, regression now only requires an $m times n$m\texttimes{}n matrix. Finally, in a special case, the hyperparameter optimization is completely independent from the number of training samples. The proposed method can regress over multiple outputs, learn the correlations between them, and estimate their derivatives to any order. The computational complexity reduction, regression capabilities, multioutput correlation learning, and comparison to the state of the art are demonstrated in simulation examples. Finally we show how the proposed approach can be utilized to model real human data.},
journal = {IEEE Intelligent Systems},
month = jul,
pages = {56–69},
numpages = {14}
}

@inproceedings{fitc,
 author = {Snelson, Edward and Ghahramani, Zoubin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 pages = {},
 publisher = {MIT Press},
 title = {Sparse Gaussian Processes using Pseudo-inputs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},
 volume = {18},
 year = {2005}
}

@misc{fitc-heteroskedasticity,
      title={Understanding Probabilistic Sparse Gaussian Process Approximations}, 
      author={Matthias Bauer and Mark van der Wilk and Carl Edward Rasmussen},
      year={2017},
      eprint={1606.04820},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1606.04820}, 
}


@InProceedings{vfe,
  title = 	 {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
  author = 	 {Titsias, Michalis},
  booktitle = 	 {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {567--574},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/titsias09a.html},
  abstract = 	 {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.}
}

@Misc{gpy,
  author =   {{GPy}},
  title =    {{GPy}: A Gaussian process framework in python},
  howpublished = {\url{http://github.com/SheffieldML/GPy}},
  year = {since 2012}
}

@misc{svgp,
      title={Gaussian Processes for Big Data}, 
      author={James Hensman and Nicolo Fusi and Neil D. Lawrence},
      year={2013},
      eprint={1309.6835},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1309.6835}, 
}

@article{foreman-mackay,
   author = {{Foreman-Mackey}, D. and {Agol}, E. and {Ambikasaran}, S. and
            {Angus}, R.},
    title = "{Fast and Scalable Gaussian Process Modeling with Applications to
              Astronomical Time Series}",
  journal = {aj},
     year = 2017,
    month = dec,
   volume = 154,
    pages = {220},
      doi = {10.3847/1538-3881/aa9332},
   adsurl = {http://adsabs.harvard.edu/abs/2017AJ....154..220F},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{matching-pursuit,
 author = {Keerthi, Sathiya and Chu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 pages = {},
 publisher = {MIT Press},
 title = {A matching pursuit approach to sparse Gaussian process regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/1ec3e7af38e33222bde173fecaef6bfa-Paper.pdf},
 volume = {18},
 year = {2005}
}

@unpublished{galaxy-spectra-101,
  author       = {Vivienne Wild},
  title        = {Fitting Galaxy Spectra 101},
  school       = {University of St Andrews},
  note         = {Unpublished manuscript, created 4 July 2025},
  year         = {2025},
}

@phdthesis{galaxy-gp-noise,
  author       = {Ho-Hin Leung},
  title        = {Unveiling the evolutionary history of local post-starburst galaxies through careful Bayesian modelling of their spectra},
  school       = {University of St Andrews},
  year         = {2025},
  doi          = {10.17630/sta/1254},
  url          = {https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/31630/Thesis-Ho-Hin-Leung-complete-version.pdf?sequence=5&isAllowed=y},
  license      = {Creative Commons Attribution 4.0 International},
  note         = {PhD thesis}
}

@article{fitc-vfe-unifier,
  author  = {Thang D. Bui and Josiah Yan and Richard E. Turner},
  title   = {A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {104},
  pages   = {1--72},
  url     = {http://jmlr.org/papers/v18/16-603.html}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@article{fast-exp,
  title = {Class of Fast Methods for Processing Irregularly Sampled or Otherwise Inhomogeneous One-Dimensional Data},
  author = {Rybicki, George B. and Press, William H.},
  journal = {Phys. Rev. Lett.},
  volume = {74},
  issue = {7},
  pages = {1060--1063},
  numpages = {0},
  year = {1995},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.74.1060},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.74.1060}
}

@article{general-exp,
   title={A STOCHASTIC MODEL FOR THE LUMINOSITY FLUCTUATIONS OF ACCRETING BLACK HOLES},
   volume={730},
   ISSN={1538-4357},
   url={http://dx.doi.org/10.1088/0004-637X/730/1/52},
   DOI={10.1088/0004-637x/730/1/52},
   number={1},
   journal={The Astrophysical Journal},
   publisher={American Astronomical Society},
   author={Kelly, Brandon C. and Sobolewska, Małgorzata and Siemiginowska, Aneta},
   year={2011},
   month=mar, pages={52} 
}

@ARTICLE{sho-spectral-density,
       author = {{Anderson}, Edwin R. and {Duvall}, Jr., Thomas L. and {Jefferies}, Stuart M.},
        title = "{Modeling of Solar Oscillation Power Spectra}",
      journal = {apj},
     keywords = {Power Spectra, Solar Oscillations, Spectrum Analysis, Least Squares Method, Maximum Likelihood Estimates, Parameter Identification, Probability Density Functions, Solar Physics, NUMERICAL METHODS, SUN: OSCILLATIONS},
         year = 1990,
        month = dec,
       volume = {364},
        pages = {699},
          doi = {10.1086/169452},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1990ApJ...364..699A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{sho-noise-astro,
	author = {{Kallinger, T.} and {De Ridder, J.} and {Hekker, S.} and {Mathur, S.} and {Mosser, B.} and {Gruberbauer, M.} and {García, R. A.} and {Karoff, C.} and {Ballot, J.}},
	title = {The connection between stellar granulation and oscillation as seen by the Kepler mission},
	DOI= "10.1051/0004-6361/201424313",
	url= "https://doi.org/10.1051/0004-6361/201424313",
	journal = {A\&A},
	year = 2014,
	volume = 570,
	pages = "A41",
	month = "",
}

@article{orny,
  title = {On the Theory of the Brownian Motion},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  journal = {Phys. Rev.},
  volume = {36},
  issue = {5},
  pages = {823--841},
  numpages = {0},
  year = {1930},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.36.823},
  url = {https://link.aps.org/doi/10.1103/PhysRev.36.823}
}

@inproceedings{naive-nystrom,
 author = {Williams, Christopher and Seeger, Matthias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf},
 volume = {13},
 year = {2000}
}

@misc{gpytorch,
      title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration}, 
      author={Jacob R. Gardner and Geoff Pleiss and David Bindel and Kilian Q. Weinberger and Andrew Gordon Wilson},
      year={2021},
      eprint={1809.11165},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.11165}, 
}

@ARTICLE{gpflow,
  author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and
	Fujii, Keisuke. and {Boukouvalas}, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and
	Ghahramani, Zoubin and Hensman, James},
    title = "{{GP}flow: A {G}aussian process library using {T}ensor{F}low}",
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  month = {apr},
  volume  = {18},
  number  = {40},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v18/16-537.html}
}


@InProceedings{ski,
  title = 	 {Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)},
  author = 	 {Wilson, Andrew and Nickisch, Hannes},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1775--1784},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/wilson15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/wilson15.html},
  abstract = 	 {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.}
}


@InProceedings{ski-smoother,
  title = 	 {Scalable {G}aussian Processes with Grid-Structured Eigenfunctions ({GP}-{GRIEF})},
  author =       {Evans, Trefor and Nair, Prasanth},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1417--1426},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/evans18a/evans18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/evans18a.html},
  abstract = 	 {We introduce a kernel approximation strategy that enables computation of the Gaussian process log marginal likelihood and all hyperparameter derivatives in O(p) time. Our GRIEF kernel consists of p eigenfunctions found using a Nystrom approximation from a dense Cartesian product grid of inducing points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor products, computational complexity of the training procedure can be practically independent of the number of inducing points. This allows us to use arbitrarily many inducing points to achieve a globally accurate kernel approximation, even in high-dimensional problems. The fast likelihood evaluation enables type-I or II Bayesian inference on large-scale datasets. We benchmark our algorithms on real-world problems with up to two-million training points and 10^33 inducing points.}
}
