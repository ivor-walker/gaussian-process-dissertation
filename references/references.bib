@book{bayes,
	author = {Hoff, Peter D.},
	title = {A First Course in Bayesian Statistical Methods},
	year = {2009},
	isbn = {0387922997},
	publisher = {Springer Publishing Company, Incorporated},
	edition = {1st},
	abstract = {This book provides a compact self-contained introduction to the theory and application of Bayesian statistical methods. The book is accessible to readers havinga basic familiarity with probability, yet allows more advanced readers to quickly grasp the principles underlying Bayesian theory and methods. The examples and computer code allow the reader to understand and implement basic Bayesian data analyses using standard statistical models and to extend the standard models to specialized data analysis situations. The book begins with fundamental notions such as probability, exchangeability and Bayes' rule, and ends with modern topics such as variable selection in regression, generalized linear mixed effects models, and semiparametric copula estimation. Numerous examples from the social, biological and physical sciences show how to implement these methodologies in practice. Monte Carlo summaries of posterior distributions play an important role in Bayesian data analysis. The open-source R statistical computing environment provides sufficient functionality to make Monte Carlo estimation very easy for a large number of statistical models and example R-code is provided throughout the text. Much of the example code can be run as is' in R, and essentially all of it can be run after downloading the relevant datasets from the companion website for this book.}
}

@book{calculus,
	title     = "Calculus: Early Transcendentals",
	author    = "Stewart, James and Clegg, Daniel K and Watson, Saleem",
	publisher = "Cengage Learning",
	edition   =  9,
	month     =  jan,
	year      =  2020,
	address   = "Taipei, Taiwan",
	language  = "en"
}

@book{gp-ml,
    author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
    title = {Gaussian Processes for Machine Learning},
    publisher = {The MIT Press},
    year = {2005},
    month = {11},
    isbn = {9780262256834},
    doi = {10.7551/mitpress/3206.001.0001},
    url = {https://doi.org/10.7551/mitpress/3206.001.0001},
    eprint = {https://direct.mit.edu/book-pdf/2514321/book\_9780262256834.pdf},
}

@book{gp-dynamic-systems,
	title     = "Modelling and control of dynamic systems using Gaussian process models",
	author    = "Kocijan, J",
	publisher = "Springer International Publishing",
	series    = "Advances in industrial control",
	edition   =  1,
	month     =  nov,
	year      =  2015,
	address   = "Cham, Switzerland",
	copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
	language  = "en"
}

@book{linalg,
	title     = "Introduction to Linear Algebra",
	author    = "Strang, Gilbert",
	publisher = "Wellesley-Cambridge Press",
	edition   =  4,
	month     =  feb,
	year      =  2009,
	address   = "Wellesley, MA"
}

@book{maths-phys,
	title     = "Mathematical methods for physicists",
	author    = "Arfken, George B and Weber, Hans J and Harris, Frank E",
	publisher = "Academic Press",
	edition   =  6,
	month     =  jul,
	year      =  2005,
	address   = "San Diego, CA"
}

@misc{notes,
	title = "Notes",
	author = "Walker, I",
	year = 2025,
	url = {https://github.com/ivor-walker/gaussian-process-dissertation/blob/e94f234a4dfe23d8bea467f7be14d642952b0092/notes/notes.pdf}
}

@ARTICLE{big-data,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  keywords={Kernel;Scalability;Sparse representation;Complexity theory;Computational modeling;Ground penetrating radar;Predictive models;Big data;Gaussian process regression (GPR);local approximations;scalability;sparse approximations},
  doi={10.1109/TNNLS.2019.2957109}
}

@article{materials,
  title={Gaussian process regression for materials and molecules},
  author={Deringer, Volker L and Bart{\'o}k, Albert P and Bernstein, Noam and Wilkins, David M and Ceriotti, Michele and Cs{\'a}nyi, G{\'a}bor},
  journal={Chemical Reviews},
  volume={121},
  number={16},
  pages={10073--10141},
  year={2021},
  publisher={ACS Publications}
}

@article{cosmography,
  title = {Gaussian process cosmography},
  author = {Shafieloo, Arman and Kim, Alex G. and Linder, Eric V.},
  journal = {Phys. Rev. D},
  volume = {85},
  issue = {12},
  pages = {123530},
  numpages = {9},
  year = {2012},
  month = {6},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.85.123530},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.85.123530}
}

@article{vgp,
  title={The variational Gaussian process},
  author={Tran, Dustin and Ranganath, Rajesh and Blei, David M},
  journal={arXiv preprint arXiv:1511.06499},
  year={2015}
}

@article{gprn,
  title={Gaussian process regression networks},
  author={Wilson, Andrew Gordon and Knowles, David A and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1110.4411},
  year={2011}
}

@article{emulators,
  title={Diagnostics for Gaussian process emulators},
  author={Bastos, Leonardo S and O’hagan, Anthony},
  journal={Technometrics},
  volume={51},
  number={4},
  pages={425--438},
  year={2009},
  publisher={Taylor \& Francis}
}

@inproceedings{freeform-kernels,
 author = {Bonilla, Edwin V and Chai, Kian and Williams, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-task Gaussian Process Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/66368270ffd51418ec58bd793f2d9b1b-Paper.pdf},
 volume = {20},
 year = {2007}
}


@InProceedings{deriving-kernels,
  title = 	 {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  author = 	 {Wilson, Andrew and Adams, Ryan},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1067--1075},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {6},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wilson13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wilson13.html},
  abstract = 	 {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}

@inproceedings{hierarchical-kernels,
 author = {Schwaighofer, Anton and Tresp, Volker and Yu, Kai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Learning Gaussian Process Kernels via Hierarchical Bayes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
 volume = {17},
 year = {2004}
}

@inproceedings{multi-kernels,
  title={Multi-kernel Gaussian processes},
  author={Melkumyan, Arman and Ramos, Fabio},
  booktitle={IJCAI Proceedings-International Joint Conference on Artificial Intelligence},
  volume={22},
  number={1},
  pages={1408},
  year={2011}
}

@article{learning-kernels,
  title={Learning “best” kernels from data in Gaussian process regression. With application to aerodynamics},
  author={Akian, J-L and Bonnet, Luc and Owhadi, Houman and Savin, {\'E}ric},
  journal={Journal of Computational Physics},
  volume={470},
  pages={111595},
  year={2022},
  publisher={Elsevier}
}

@Manual{gaupro,
    title = {GauPro: Gaussian Process Fitting},
    author = {Collin Erickson},
    year = {2025},
    note = {R package version 0.2.15.9000, commit 6c7a7a8055506229b6d02650aca9664a84f640f8},
    url = {https://github.com/CollinErickson/GauPro},
}

@ARTICLE{signals-processing,
  author={Perez-Cruz, Fernando and Van Vaerenbergh, Steven and Murillo-Fuentes, Juan Jose and Lazaro-Gredilla, Miguel and Santamaria, Ignacio},
  journal={IEEE Signal Processing Magazine}, 
  title={Gaussian Processes for Nonlinear Signal Processing: An Overview of Recent Advances}, 
  year={2013},
  volume={30},
  number={4},
  pages={40-50},
  abstract={Gaussian processes (GPs) are versatile tools that have been successfully employed to solve nonlinear estimation problems in machine learning but are rarely used in signal processing. In this tutorial, we present GPs for regression as a natural nonlinear extension to optimal Wiener filtering. After establishing their basic formulation, we discuss several important aspects and extensions, including recursive and adaptive algorithms for dealing with nonstationarity, low-complexity solutions, non-Gaussian noise models, and classification scenarios. Furthermore, we provide a selection of relevant applications to wireless digital communications.},
  keywords={Machine learning;Learning systems;Gaussian processes;Nonlinear estimation;Noise measurement;Wiener filters;Adaptive algorithms},
  doi={10.1109/MSP.2013.2250352},
  ISSN={1558-0792},
  month={7},
}

@article{choosing-kernels,
author = {Akian, Jean-Luc and Bonnet, Luc and Owhadi, Houman and Savin, Eric},
year = {2022},
month = {08},
pages = {111595},
title = {Learning “best” kernels from data in Gaussian process regression. With application to aerodynamics},
volume = {470},
journal = {Journal of Computational Physics},
doi = {10.1016/j.jcp.2022.111595}
}

@misc{additive-kernels,
      title={Additive Covariance Kernels for High-Dimensional Gaussian Process Modeling}, 
      author={Nicolas Durrande and David Ginsbourger and Olivier Roustant and Laurent Carraro},
      year={2011},
      eprint={1111.6233},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1111.6233}, 
}


@InProceedings{random-subsampling,
  title = 	 {On Random Subsampling of Gaussian Process Regression: A Graphon-Based Analysis},
  author =       {Hayashi, Kohei and Imaizumi, Masaaki and Yoshida, Yuichi},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2055--2065},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/hayashi20a/hayashi20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/hayashi20a.html},
  abstract = 	 {In this paper, we study random subsampling of Gaussian process regression, one of the simplest approximation baselines, from a theoretical perspective. Although subsampling discards a large part of training data, we show provable guarantees on the accuracy of the predictive mean/variance and its generalization ability.For analysis, we consider embedding kernel matrices into graphons, which encapsulate the difference of the sample size and enables us to evaluate the approximation and generalization errors in a unified manner. The experimental results show that the subsampling approximation achieves a better trade-off regarding accuracy and runtime than the ystrom and random Fourier expansion methods.}
}

@article{sparse-kernels,
author = {Gneiting, Tilmann},
year = {2002},
month = {11},
pages = {493-508},
title = {Compactly Supported Correlation Functions},
volume = {83},
journal = {Journal of Multivariate Analysis},
doi = {10.1006/jmva.2001.2056}
}
