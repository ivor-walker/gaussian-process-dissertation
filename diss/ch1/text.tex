\begin{document}

\section{Defining the Gaussian Process}



\subsection{Weight-space view \cite{gp-ml}}

\subsubsection{Standard linear model}
The standard linear model summises that we have some data-generating function $f(.)$ that linearily combines training data $X$ and parameters of some model $W$ to produce an output $y$:
\begin{equation} \label{eq:linear_model}
    \begin{aligned}
        y = f(X) + \epsilon \\
        f(X) = X^T W \\
    \end{aligned}
\end{equation}

We add a noise term $\epsilon$ because $y$ is rarely a perfect observation of $f(X)$ (e.g. measurement error). The standard linear model assumes that $\epsilon$ is drawn from a Gaussian distribution $\epsilon \sim N(0, \sigma_n^2I)$. We add a covariance matrix $I$ to describe how the noise for one observation is related to the noise of another observation.

We can combine our expressions and assumptions for $f(X)$ and $\epsilon$ to produce a conditional distribution. Effectively a distribution of errors, this is the distribution from which $y$ is drawn from after knowing perfectly $X$ and $W$:
\begin{equation*}
    p(y|X,W) = \mathcal{N}(y | X^TW, \sigma^2_nI)
\end{equation*}


\subsubsection{Determining weights}
Our first task is to find $W$ as we typically do not know these in advance. Frequentist approaches focus on arriving at a single estimate of $W$ ($\hat{W}$) via "maximum likelihood estimation" (MLE). $p(y|X,W)$ is at its highest density around the expected value $y | X^TW$ so we can use optimisation methods to find the $\hat{W}$ at the maximum of $p(y|X,W)$. Because we assume $E[p(y|X,W)] = 0$, the values of $W$ at the maximum of $p(y|X,W)$ are also the values of $W$ that pushes the squared error $||y - X^TW||^2$ closest to zero. We can communicate uncertainty surrounding $\hat{W}$ by computing "standard errors", or the ratio between the variance of our errors and the variance of $X$. A high variance in errors shows that $\hat{W}$ i, but a broader range of $X$ makes it easier to estimate $W$.

Instead of producing point estimates for $\hat{W}$ and uncertainty, Bayesian statistics treats $W$ as a random variable and specifies an expected value and a variance. Placing $W$ in a probabilistic framework allows us to propagate uncertainty throughout the model and to encode beliefs (e.g. from domain experts) about the weights before observing the data.

We start with a "prior" distribution of $W$, which the Bayesian linear model assumes:
\begin{equation} \label{eq:prior_distribution}
    p(W) \sim N(0, \Sigma_p)
\end{equation}
Then, we observe the data and update our beliefs about the weights using Bayes' theorem to produce a "posterior" distribution $p(W|X,y)$.

\begin{equation*}
    p(W|X,y) = \frac{p(y|X,W)p(W)}{p(y|X)}
\end{equation*}
$p(y|X,W)$ is the density of the residuals after applying $p(W)$ to $X,W$ under our assumed noise model $\epsilon$, and $p(y|X)$ is the marginal likelihood - how likely the data is given the model.   

\paragraph{Deriving our posterior}
To understand the relationship between $p(W|X,y)$ and $W$, we can ignore terms that do not vary with $W$ (e.g. our marginal likelihood) by absorbing them into the proportionality constant:
\begin{equation} \label{eq:posterior}
    p(W|X, y) \propto p(y|X,W)p(W)
\end{equation}

% We can write $p(Y|X,W)$ as the distribution of errors for each datapoint $i$:
% \begin{equation*}
%     p(y|X,W) = \prod_{i=1}^N \mathcal{N}(y_i|X^TW, \sigma^2_n)
% \end{equation*}

TODO fix
We can get a probability density function (PDF) for our error distribution by representing $Y | X^TW$ in squared error form and substituting it into the Gaussian PDF:
\begin{equation} \label{eq:likelihood}
    p(y|X,W) = exp\left(-\frac{1}{2\sigma^2_n}||y -X^TW||^2\right)
\end{equation}

Reframing $p(W)$ as a PDF:
 \begin{equation*}
     p(W) = \frac{1}{[\sqrt{\sigma_p}]\sqrt{2\pi}} exp\left(-\frac{1}{2}\frac{([W]-[0])}{[\Sigma_p]}\right)
\end{equation*}

The first term can be absorbed into the proportionality constant. Rewriting the second term as a negative exponential:
\begin{equation} \label{eq:prior_pdf}
    p(W) \propto exp\left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)
\end{equation}

Putting both expressions for \ref{eq:likelihood} and \ref{eq:prior_pdf} into \ref{eq:posterior}:
\begin{equation*}
    p(W|X,y) \propto exp\left(-\frac{1}{2\sigma^2_n}||y -X^TW||^2\right)exp\left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)
\end{equation*}

Expanding $||y - X^TW||^2$ to $y^Ty - 2y^TXW + W^TX^TXW$:
\begin{equation*}
    p(W|X,y) \propto exp\left(-\frac{1}{2\sigma^2_n}(y^Ty - 2y^TXW + W^TX^TXW)\right)exp\left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)
\end{equation*}

Putting both exponentials together by adding their powers:
\begin{equation*}
    p(W|X,y) \propto exp\left(\frac{1}{\sigma^2_n}(y^Ty - 2y^TXW + W^TX^TXW) + \left(-\frac{1}{2}W^T\Sigma_p^{-1}W\right)\right)
\end{equation*}

Rearranging the inside term to be a quadratic, linear and constant term in $W$:
\begin{equation*}
    p(W|X,y) \propto exp\left(\frac{1}{2}W^T\left(\frac{1}{\sigma^2_n}X^TX + \Sigma_p^{-1}\right)W - \left(\frac{1}{\sigma^2_n}y^TX\right)W + \frac{1}{2}y^Ty\right)
\end{equation*}

We can ignore the constant final term. Introducing these terms to simplify this result:
\begin{equation} \label{eq:A_b}
    \begin{aligned}
        A = \Sigma_p^{-1} + \frac{1}{\sigma^2_n}X^TX \\
        b = \frac{1}{\sigma^2_n}y^TX \\
        p(W|X,y) \propto exp\left(-\frac{1}{2}W^TAW + b^TW\right) \\
    \end{aligned}
\end{equation}

\paragraph{Deriving the properties of the posterior by completing the square}
Now we have a simplified form of the posterior's PDF, we need to get it into a Gaussian form to recover the properties of the posterior distribution.

Bringing all terms inside the exponential to a single term:
\begin{equation*}
    -\frac{1}{2}W^TAW + b^TW = \frac{1}{2}\left(-W^TAW + 2b^TW\right)
\end{equation*}

Completing the square on our new inner term $W^TAW - 2b^TW$ 
\begin{equation} \label{eq:posterior_pdf}
    \begin{aligned}
        W^TAW - 2b^TW = (W - A^{-1}b)^TA(W - A^{-1}b) - b^TA^{-1}b
        p(W|X,y) \propto exp\left(-\frac{1}{2}\left((W - A^{-1}b)^TA(W - A^{-1}b) - b^TA^{-1}b\right)\right)
    \end{aligned}
\end{equation}

Looking at the Gaussian PDF:
\begin{equation} \label{eq:gaussian_pdf}
    N(W | \mu, \Sigma) = \frac{1}{\sqrt{(2\pi)^d |\Sigma|}} exp\left(-\frac{1}{2}(W - \mu)^T\Sigma^{-1}(W - \mu)\right)
\end{equation}

Our expression lines up with the Gaussian PDF's "kernel" term $exp\left(-\frac{1}{2}(W - \mu)^T\Sigma^{-1}(W - \mu)\right)$, where $\mu = A^{-1}b$ and $\Sigma = A^{-1}$ ($\Sigma^{-1} = A$). Therefore, \ref{eq:posterior_pdf} can be represented as a Gaussian distribution:
\begin{equation} \label{eq:posterior_gaussian}
    p(W|X,y) \sim N(A^{-1}b, A^{-1})
\end{equation}

Inside our definition of $A$ at $\ref{A_b}$, we are missing an expression for $\Sigma_p$. Assuming independence of noise under the linear model, our weight variance $\Sigma_p$ under the Bayesian linear model is "isotropic", meaning it is the same in all directions.
\begin{equation*}
    \Sigma_p = \tau^2 I
\end{equation*}
Because we assume independence, $I$ is an "identity matrix where each diagonal element is 1 and all off-diagonal elements are 0. $\tau^2$ is a scalar variance term, chosen as a prior.

Substituting the isotropic prior $\Sigma_p$ into $A$:
\begin{equation*}
    A = \Sigma_p^{-1} + \frac{1}{\sigma^2_n}X^TX = \left[{\tau^2}I\right]^{-1} + \frac{1}{\sigma^2_n}X^TX =  \frac{1}{\tau^2}I + \frac{1}{\sigma^2_n}X^TX
\end{equation*}

Simplifying:
\begin{equation} \label{eq:A_isotropic}
A = \frac{1}{\sigma_n^2}\left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)
\end{equation}

\paragraph{Gaussian posteriors and ridge regression}
So far we have worked exclusively within the Bayesian paradigm, but we can draw some value of $W$ from our posterior distribution to relate it to a frequentist framework. For Gaussian posteriors, our expected value of $W$ $A^{-1}b$ is also its mode. This is called the maximum a posteriori (MAP) estimate of W, and is due to symmetries in linear model and posterior and is not the case in general.  Our MAP estimate does not matter within the Bayesian framework but is equivelant to our frequentist $\hat{W}$.

Substituting our full expressions for $A$ \ref{eq:A_isotropic} and $b$ \ref{eq:A_b} into our MAP estimation:
\begin{equation*} 
    W_{\text{MAP}} = A^{-1}b = \left[\frac{1}{\sigma_n^2}(X^TX + \frac{\sigma_n^2}{\tau^2}I\right]^{-1} \cdot \left[\frac{1}{\sigma_n^2}y^TX\right] \\
\end{equation*}

Inverting LHS term of $A$:
\begin{equation*}
    A^{-1} = \frac{\sigma_n^2}{X^TX + \frac{\sigma_n^2}{\tau^2}I} = \sigma_n^2\left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)^{-1}
\end{equation*}

Substituting this back into $W_\text{MAP}$ cancels out the $\sigma_n^2$ term in A with the $\frac{1}{\sigma_n^2}$ term in B:
\begin{equation*}
    W_{\text{MAP}} = \sigma_n^2\left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)^{-1} \cdot \frac{1}{\sigma_n^2}y^TX = \left(X^TX + \frac{\sigma_n^2}{\tau^2}I\right)^{-1} \cdot y^TX
\end{equation*}

This is equivelant to the solution to ridge regression, where $\lambda = \frac{\sigma_n^2}{\tau^2}$.
\begin{equation*}
    W_{\text{ridge}} = \left(X^TX + \lambda I\right)^{-1}X^Ty
\end{equation*}

Ridge regression introduces some bias to lower the variance in a frequentist linear model by shrinking weights, where the $\lambda$ term controls the amount of shrinkage applied to the weights. This is traditionally useful where variance is particularly high (e.g. multicollinearity) and can be reduced at the cost of little bias.  

Our MAP estimation in Bayesian linear regression with isotropic priors is equivalent to ridge regression, where the amount of bias we introduce depends on our confidence in our priors. The more we trust our prior, the higher our $\lambda$ and the more we shrink our weights towards zero. A lower $\tau$ means we are more confident in $p(W)$ and have better priors, whereas a higher $\sigma_n$ means lower confidence in $p(y|X,W)$ and worse weights that should be shrunk closer to zero.
        
\subsubsection{Predictive distribution}
\paragraph{Deriving the predictive distribution}
Our second task is to make predictions $y_*$ using new input data $X_*$ and our previously learned weights $W$. Frequentist methods simply multiply $\hat{W}$ by $X_*$, but this does not propagate uncertainty in $W$. In this Bayesian framework, we form a "predictive distribution" which we sample from to get our noise-free function evaluations $f(X_*)$ (denoted $f_*$) and add $\epsilon$ to get our noisy predictions $y_*$.

\begin{equation*}
    p(f_*|X_*,X,y) = \int p(f_*|X_*,W) \cdot p(W | X,y)dW
\end{equation*}
$p(f_*|X_*,W)$ is what we think the function looks like after producing a prediction using $X_*$ and perfect knowledge of $W$. $p(W|X,y)$ is our familiar \ref{eq:posterior_gaussian} posterior distribution of weights. $p(f_*|X_*,W) \cdot p(W|X,y)$ is the joint distribution of our predictions and our posterior weights, which gets us the conditional distribution $p(f_*,W|X_*,X,y)$ by definition of conditional probability. Because $p(f_*,W|X_*,X,y)$ relies on our perfect knowledge of $W$, which we lack, we integrate over all possible $W$ to get the final predictive distribution $p(f_*|X_*,X,y)$
    
$p(f_*|X_*,W)$ is our error distribution, which we assume to be distributed normally and independently with our $I$ identity matrix:
\begin{equation*}
    p(f_* | X_*, W) = \mathcal{N}(f_* | W^TX_*, \sigma^2_nI)
\end{equation*}

Substituting into \ref{eq:gaussian_pdf} and absorbing the LHS term into the proportionality constant:
\begin{equation*}
    p(f_*|X_*,w) \propto exp\left(-\frac{1}{2}\frac{1}{\sigma^2_n}(f_* - W^TX_*)^2\right)
\end{equation*}

Multiplying $P(f_*|X_*,W)$ and $p(W|X,y)$ to get our conditional $p(f_*, W|X_*,X,y)$, and add the exponents: 
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(\frac{1}{2}(-W^TAW + 2b^TW) + \left(-\frac{1}{2}\frac{1}{\sigma^2_n}(f_* - W^TX_*)^2\right)\right)
\end{equation*}

Combining the terms inside the exponent:
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^TAW - 2b^TW + \frac{1}{\sigma^2_n}(f_* - W^TX_*)^2\right)\right)
\end{equation*}

Expanding the squared term:
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^TAW - 2b^TW + \frac{1}{\sigma^2_n}(f_*^2 - 2f_*W^TX_* + W^TX_*X_*^TX_*)\right)\right)
\end{equation*}

Similar to our posterior, we can rearrange this to be a quadratic, linear and constant term in $W$:
\begin{equation} \label{eq:predictive_completing_square}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^T\left(A + \frac{1}{\sigma^2_n}X_*X_*^T\right)W - 2\left(b + \frac{1}{\sigma^2_n}f_*X_*\right)^TW + \frac{1}{\sigma_n^2}f_*^2\right)\right)
\end{equation}

We can define new terms $A_*$ and $b_*$ to simplify this expression:
\begin{equation*}
    \begin{aligned}
        A_* = A + \frac{1}{\sigma^2_n}X_*X_*^T \\
        b_* = b + \frac{1}{\sigma^2_n}f_*X_*
    \end{aligned}
\end{equation*}

Substituting into \ref{eq:predictive_completing_square}:
\begin{equation*}
    p(f_*,W|X_*,X,y) \propto exp\left(-\frac{1}{2}\left(W^TA_*W - 2b_*^TW + \frac{1}{\sigma_n^2}f_*^2\right)\right)
\end{equation*}

Integrating out $W$ to get our predictive distribution:
\begin{equation} \label{eq:predictive_distribution_1}
    p(f_*|X_*,X,y) = \int p(f_*,W|X_*,X,y)dW \propto \int exp\left(-\frac{1}{2}\left(W^TA_*W - 2b_*^TW + \frac{1}{\sigma_n^2}f_*^2\right)\right)dW
\end{equation}

Factoring out $\frac{1}{\sigma_n^2}f_*^2$ as it does not depend on $W$ (since $\int exp(X) dX = exp(X)$):
\begin{equation*}
    = exp\left(-\frac{1}{2}\frac{1}{\sigma_n^2}f_*^2\right) \times \int exp\left(-\frac{1}{2}\left(W^TA_*W - 2b_*^TW\right)\right) dW
\end{equation*}

Evaluating the RHS multivariate Gaussian integral:
\begin{equation*}
    \int exp\left(-\frac{1}{2} \left( W^TA_*W - 2b_*^TW \right) \right) dW = \frac{(2\pi)^{D/2}} {\sqrt{|A_*|}} exp\left( \frac{1}{2} b_*^TA_*^{-1}b_* \right)
\end{equation*}

Substituting back into \ref{eq:predictive_distribution_1}: 
\begin{equation*}
    p(f_*|X_*,X,y) \propto exp\left(-\frac{1}{2}\frac{1}{\sigma_n^2}f_*^2\right) + \frac{(2\pi)^{D/2}}{\sqrt{|A_*|}} \cdot exp\left(\frac{1}{2}b_*^TA_*^{-1}b_*\right)  
\end{equation*}
    
Now that no part of our expression is dependent on W, we need an expression for everything that depends on $f_*$. 

Absorbing the second term (since it does not depend on $f_*$) into the proportionality constant, and combining the remaining exponential terms by adding their powers:
\begin{equation*}
    p(f_*|X_*,X,y) \propto \exp\left(-\frac{1}{2}\frac{1}{\sigma_n^2}f_*^2 + \frac{1}{2}b_*^TA_*^{-1}b_*\right)
\end{equation*}

Similar to deriving properties from our posterior, we can rearrange this expression and complete the square to derive the properties of our predictive distribution:
\begin{equation} \label{eq:predictive_gaussian}
    p(f_*|X_*,W) \sim N(X_*^TA^{-1}b, X_*^TA^{-1}X_*)
\end{equation}
The variance is quadratic in $X_*$ with $A^{-1}$, showing that predictive uncertainties grow with size of $X_*$.

\subsubsection{Projections of inputs into feature space}
One problem with this model is that it assumes a linear relationship between $X$ and $y$. We can project our inputs into a higher dimensional feature space and apply a linear model in this space to express non-linear relationships between $X$ and $y$. 

Defining $\phi(X)$ as a function that maps a $D$-dimensional input vector $X$ into an $N$ dimensional feature space, our standard linear model becomes:
\begin{equation*}
    f(X) = \phi(X)^T W
\end{equation*}
For example, a scalar $x$ could be projected into the space of powers of $x$: $\phi(x) = [1, x, x^2, \ldots, x^d]^T$ for a polynomial basis expansion of degree $d$ to represent a $d$-power relationship between $x$ and $y$.
.
%   \item $\phi(x)$ must be independent of $W$ so that we can learn $W$ from the data
Substituting $\phi(X)$ for $X$ in \ref{eq:predictive_gaussian}:
\begin{equation} \label{eq:predictive_gaussian_phi}
    p(f_*|X_*,X,y) = N(\phi(X_*)^TA_{\phi}^{-1}b_{\phi} , \phi(X_*)^TA_{\phi}^{-1}\phi(X_*))
\end{equation}

Where $A_{\phi}$ and $b_{\phi}$ are now:
\begin{equation*}
    A_{\phi} = \Sigma_p^{-1} + \frac{1}{\sigma^2_n}\phi(X)^T\phi(X)
    b_{\phi} = \frac{1}{\sigma^2_n}\phi(X)^Ty
\end{equation*}

\subsubsection{Computational issues}
\paragraph{Avoiding inversion of $A_{\phi}$}
\ref{eq:predictive_gaussian_phi} requires inverting the $N \times N$ matrix $A_{\phi}$, where $N$ is dimension of feature space, to get the expected value and variance. 

Typically, matrices are inverted using Gaussian elimination. We need to perform a "forward" pass which requires $N$ pivots on every row and column, $N$ eliminations per pivot, and up to $2N$ columns to update, resulting in an $O(N^3)$ time complexity. Then, we need to perform a backwards pass in the opposite direction which is another $O(N^3)$ operation. Finally, we need to multiply the inverse by the RHS vector $b_{\phi}$, which is an $O(N^2)$ operation but appears trivial next to these two cubic steps. 

We can mitigate this for a particular class of high-dimensional $N > n$ problems by restating the predictive distribution in terms of the number of training data points $n$ which would require inverting an $n \times n$ matrix instead. For polynomial basis expansions, $N$ is degree $D$ multiplied by number of features, so $N$ can be very large or even infinite (e.g. SE). % Some data domains (e.g. text classification, genomic data) have very high dimensional feature spaces.

Substituting $b_{\phi}$ into our predictive distribution mean:
\begin{equation*}
    \mathbb{E}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*)^T \cdot A_\phi^{-1} \cdot \left[\frac{1}{\sigma_n^2}\phi(X)^Ty\right]
\end{equation*}

Rearranging to isolate $A_\phi^{-1}\phi(X)$
\begin{equation*}
    = \frac{1}{\sigma_n^2}\left[A_{\phi}^{-1}\phi(X)\right]^Ty
\end{equation*}

We can use the Sherman-Morrison identity to get an expression for $A_{\phi}^{-1}$ directly, where $K =\phi(X)^T\Sigma_p\phi(X)$
\begin{equation*}
    A_{\phi}^{-1} = \Sigma_p - \Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p
\end{equation*}

For the mean, we can use the Sherman-Morrison identity again to get an expression for $A_{\phi}^{-1}\phi(X)$
\begin{equation} \label{eq:A_phi_inverse_phi_X}
    A_{\phi}^{-1}\phi(X) = \sigma_n^2\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}
\end{equation}

Substitute in \ref{eq:A_phi_inverse_phi_X} into our \ref{eq:predictive_gaussian_phi}:
\begin{equation*}
    \mathbb{E}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*) \frac{1}{\sigma_n^2}\left[\sigma_n^2\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\right]^Ty
\end{equation*}

$\frac{1}{\sigma_n^2}$ and $\sigma_n^2$ cancel out, leaving us with this final expression for the mean:
\begin{equation} \label{eq:alt_predictive_mean_phi}
    \mathbb{E}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*)^T \cdot \Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}y
\end{equation}

For the variance, we cannot use the Sherman-Morrison identity to arrive at an expression for $A_{\phi}^{-1}\phi(X_*)$ because $\phi(X_*)$ is an arbitrary N-vector, not one of the columns of $\phi(X)$. Instead, we use the $A_{\phi}^{-1}$ expression we derived earlier to get an expression for $A_{\phi}^{-1}\phi(X_*)$:
\begin{equation*}
    A_{\phi}^{-1}\phi(X_*) = \Sigma_p \cdot \phi(X_*) - \Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p \cdot \phi(X_*)
\end{equation*}

 Substituting this into \ref{eq:predictive_gaussian_phi}:
\begin{equation} \label{eq:alt_predictive_variance_phi}
    \text{Var}_{p(f_*|X_*,X,y)}[f_*] = \phi(X_*)^T\Sigma_p\phi(X_*) - \phi(X_*)^T\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p\phi(X_*)
\end{equation}

With our alternative mean \ref{eq:alt_predictive_mean_phi} and variance \ref{eq:alt_predictive_variance_phi}, we can form an alternative expression for our predictive distribution:
\begin{equation} \label{eq:alt_predictive_gaussian_phi}
    \begin{aligned}
        p(f_*|X_*,X,y) = \mathcal{N}( \\
        \phi(X_*)^T\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}y , \\
        \phi(X_*)^T\Sigma_p\phi(X_*) - \phi(X_*)^T\Sigma_p\phi(X)(K+\sigma_n^2I)^{-1}\phi(X)^T\Sigma_p\phi(X_*) \\
        )
    \end{aligned}
\end{equation}

With this alternative formulation, we need to invert the $n \times n$ matrix $K + \sigma_n^2I$ only. Geometrically, $n$ datapoints can span at most $n$ dimensions in the feature space - if $N > n$, the data forms a subspace of the feature space.

\paragraph{Kernels and the kernel trick}
In \ref{eq:alt_predictive_gaussian_phi}, $\phi(.)$ is always an inner product of a positive definite correlation matrix $\Sigma_p$, but with different arrangements of $\phi(X)$ and $\phi(X_*)$. We can define $k(X,X') = \phi(X)^T\Sigma_p\phi(X')$ as a covariance function or kernel, where $X$ and $X'$ are either $X$ or $X_*$. For example, in \ref{eq:alt_predictive_gaussian_phi} the definition of $K = \phi(X)^T\Sigma_p\phi(X)$ becomes $K = k(X,X)$.

Introducing $\psi(X)$ to better represent $k(X,X')$ as an inner product:
\begin{equation*}
    \begin{aligned}
        \psi(X) = \phi(X)\Sigma_p^{1/2} \\
        k(X,X') = \psi(X)^T\psi(X') \\
    \end{aligned}
\end{equation*}
These inner product representations require us to compute $\phi(X)$ and $\phi(X')$ in the feature space. A higher-dimensional feature space requires more compute to evaluate $\phi(X)$ and more memory to store $\phi(X)$ and $\phi(X')$.

Instead, the representer theorem guarantees that we can find an equivelant kernel that does not require us to explicitly compute $\phi(X)$ or $\phi(X')$ in the feature space. With this "kernel trick" we avoid the associated memory and computational costs of explicitly computing $\phi(X)$ and $\phi(X')$. Since computing the kernel directly is more convenient than the feature vectors themselves, these kernels become the object of primary interest.

For example, if we had some polynomial transformation $\phi(X) = [1, x^1, ..., x^D]^T$ and $\Sigma_p$ as an identity matrix, we could define $k(X,X')$ as inner products:
\begin{equation*}
    \begin{aligned}
        \psi(X) = [1, x^1, ..., x^D]^T
        k(X,X') = \psi(X)^T\psi(X')
    \end{aligned}
\end{equation*}
This approach requires arranging $\phi$ and $\phi(X')$ into a $D$ sized vector, then taking the dot product. This is trivial for small $D$, but as $D$ becomes infinite (e.g. RBF kernel), arranging a $D$ sized vector requires too much memory and the dot product becomes computationally expensive.

Instead, we can define $k(X,X')$ as an equivelant function of $X$ and $X'$ directly:
\begin{equation*}
    k(X,X') = (1 + X \cdot X')^D
\end{equation*}
This is the polynomial kernel, which is equivalent to our original polynomial basis expansion $\phi(X)$ without explicitly computing $\phi(X)$.



\subsection{Function-space view \cite{gp-ml}}

\subsubsection{Gaussian processes (GP)}

\paragraph{Bayesian linear model}
We can define our Bayesian linear model of a real process $f(X)$ entirely in terms of mean function $m(X)$ and covariance function $k(X,X')$:
\begin{equation*}
    \begin{aligned}
        m(X) = \phi(X)^T\mathbb{E}[W] = \phi(X)^T[0] = 0 \\
        k(X,X') = \phi(X)^T\mathbb{E}[WW^T]\phi(X') = \phi(X)^T\Sigma_P\phi(X')
    \end{aligned}
\end{equation*}

Our covariance function here is in inner product form. The kernel trick here uses the squared exponential (SE) covariance function, also known as the radial basis function (RBF) or Gaussian kernel:
\begin{equation*}
    k(f(X), f(X')) = \exp\left(-\frac{1}{2}\frac{|X - X'|^2}{l^2}\right)
\end{equation*}
It can be shown that SE corresponds to a Bayesian linear regression model with infinite basis functions. 

% For SE, covariance is almost unity between outputs whose inputs are close together, but decays exponentially as inputs get further apart.   The Mercer theorem states that for every positive definite covariance function $k(X,X')$, there exists a possibly infinite set of basis functions. SE can also be obtained from the linear combination of infinite Gaussian-shaped basis functions. Because SE is infinitely differentiable, it produces smooth functions.


\paragraph{Function evaluations to a random function}
We can choose a subset $X_{*1}$ from our test data $X_*$ and apply it to our model get some function evaluations $f(X_{*1})$. $f(X_{*1})$ can be described as a multivariate Gaussian distribution, e.g. in the Bayesian linear model $f(X_{*1}) \sim N(0, k(X_{*1}, X_{*1}))$. Each output $f(X_{\theta*1})$ in our $f(X_{*1})$ vector is a random variable with mean $0$ and covariance with each other $K_{\theta\theta'} = k(X_{\theta*}, X_{\theta'*})$. There exists some random function $g(X_{*1})$ for our subsets such that $f(X_{*1}) = g(X_{*1})$. We only know the value of $g(X_{*1})$ at the points $X_{*1}$, so $g(X_{*1}) = {X_{*1} : f(X_{*1})}$. Because $g(X)$ entirely consists of random points, we can think of $g(X_{*1})$ as a random function and our distribution $f(X)$ can be seen as a distribution of these random $g(X)$ functions. We can recover our individual $g(X_{*1}$ thanks to consistency - if we marginalised out our subset from the entire distribution $f(X_*)$, we would recover the subset distribution $N(0, K_*(X_{*1}, X_{*1}))$ that describes our random function $g(X_{*1})$.

\paragraph{Definition of a GP}
A GP is a collection of random variables, any finite number of which have a joint Gaussian distribution. Ultimately, GPs describe a distribution of random functions where each drawn function is a $g(X)$ sample from the GP.
\begin{equation*}
    \begin{aligned}
        f(X) \sim \mathcal{GP}(m(X), k(X,X')), \\
        m(X) = \mathbb{E}[f(X)], \\
        k(X,X') = \text{Cov}(f(X), f(X')) = \mathbb{E}[(f(X) - m(X))(f(X') - m(X'))]
    \end{aligned}
\end{equation*}
% These random variables represent the value of $f(X)$ at location $X$. often Gaussian processes are defined over time so $X$ can be a time point. The covariance function specifies the covariance between pairs of random variables.

\paragraph{Consistency requirement}
This definition implies a consistency requirement - any group of functions drawn from our GP can be described by the same distribution as our GP. For example, if our GP implies that $(f(X_1), f(X_2)) \sim \mathcal{N}(\mu, \Sigma)$, then $(f(X_1) \sim \mathcal{N}(\mu_1, \Sigma) and f(X_2) \sim \mathcal{N}(\mu_2, \Sigma))$ where $\mu_{\theta} = m(X_{\theta})$ and $\Sigma_{\theta\theta} = k(X_{\theta}, X_{\theta})$. This requirement is also called the marginalisation property, because to get the smaller distribution of $f(X_1)$ we marginalise out the larger distribution of $f(X_1), f(X_2)$ by integrating the larger distribution wrt $f(X_2)$. Consistency is automatically gained if our covariance function specifies entries in a covariance matrix.
    
\subsubsection{Predictive distributions with noise-free observations}
\paragraph{Prior distribution over functions}
$f(X)$ and $f(X_*)$ are jointly distributed according to the prior:
\begin{equation*}
    \begin{pmatrix}
        f(X) \\ f(X_*)
    \end{pmatrix} \sim N\left(
    \begin{pmatrix}
        0 \\ 0
    \end{pmatrix},
    \begin{pmatrix}
        K(X,X) & K(X,X_*) \\ K(X_*,X) & K(X_*,X_*)
    \end{pmatrix}
    \right)
\end{equation*}

\paragraph{Posterior distribution of functions}
To get the posterior distribution of functions given the training data and our prior, we can condition the joint prior distribution on the training data. Intuitively, this is like generating random functions $g(X)$ and rejecting those that do not pass through the training data. Probabilistically, we condition our joint Gaussian prior distribution on the observations $p(f(X_*)|X_*, X, f(X))$.

Substituting $p(W)$ and our conditioning $X$ into the Gaussian multivariate conditioning identity:
\begin{equation*}
    \begin{aligned}
        p(f(X_*)|X_*, X, f(X)) \sim N( \\
        [0] + [K(X_*,X)][K(X,X)]^{-1}([f(X)] - [0]), \\
        [K(X*,X*)] - [K(X_*,X)][K(X,X)]^{-1}[K(X,X_*)] \\
        )
    \end{aligned}
\end{equation*}

Although we condition on $X_*$, $X$, and $f(X)$, we only substitute $f(X)$ because $X_*$ and $X$ are known constants, but $f(X)$ is random because it is a sample from the prior. We also swap $f(X_*)$ and $f(X)$ in our prior to match the conditioning identity, such that our input vector into the conditioning identity is $(f(X_*), f(X))^T$.
    
Simplifying the last term in the mean:
\begin{equation} \label{eq:conditioning}
    \begin{aligned}
        p(f(X_*)|X_*, X, f(X)) \sim N( \\
        K(X,X_*)K(X,X)^{-1}f(X), \\
        K(X_*,X_*) - K(X_*,X)K(X,X)^{-1}K(X,X_*) \\
        )
    \end{aligned}
\end{equation}


\subsubsection{Predictive distributions with noisy observations}
\paragraph{Noisy observations prior}
It is typical to not have the noise-free function evaluations $f(X)$ as our training data, but instead our noisy observations $y$. We can simply add $\epsilon$:
\begin{equation*}
    \text{Cov}(y_p, y_q) = K(X_p, X_q) + \sigma^2_n\delta_{pq}
\end{equation*}
$\delta_{pq}$ represents our independence condition in 1D. This is the Kronecker delta, which returns 1 if indices ($p$, $q$) are equal and 0 otherwise. $\sigma^2_n$ is the noise variance, which is a constant for all observations.
    
In matrix form:
\begin{equation*}
    \text{Cov}(Y) = k(X,X) + \sigma^2_nI
\end{equation*}
    
This gives us this prior:
\begin{equation*}
    \begin{pmatrix}
        Y \\ f(X_*)
    \end{pmatrix} \sim N\left(
    \begin{pmatrix}
        0 \\ 0
    \end{pmatrix},
    \begin{pmatrix}
        K(X,X) + \sigma^2_nI & K(X,X_*) \\ K(X_*,X) & K(X_*,X_*)
    \end{pmatrix}
    \right)
\end{equation*}

\paragraph{Noisy observations posterior}
As before, we can form a predictive distribution using the Gaussian multivariate conditioning identity:
\begin{equation*}
    \begin{aligned}
        p(f(X_*)|X_*, X, Y) \sim N( \\
        K(X_*,X)[K(X,X) + \sigma^2_nI]^{-1}Y, \\
        K(X_*,X_*) - K(X_*,X)[K(X,X) + \sigma^2_nI]^{-1}K(X,X_*) \\
        )
    \end{aligned}
\end{equation*}

Substituting $k(X,X') = \phi(X)^T\Sigma_p\phi(X')$ into here gives us the exact same result as \ref{eq:alt_predictive_gaussian_phi}.

Our variance is independent of the targets $y$ and only depends on our inputs $X$ and $X_*$. Our variance is two terms: our prior covariance $K(X_*,X_*)$, a term representing the information the observations give us about the function. As before, we can compute the predictive distribution of $y_*$ by adding the noise term $\sigma^2_nI$ to the variance.

\subsubsection{Marginal likelihood}
We need some measure of how well our GP fits the data, which we can get by computing the marginal likelihood $p(Y|X)$:
\begin{equation*}
    p(Y|X) = \int p(Y|f,X)p(f|X) df
\end{equation*}

$p(y|f,X)$ is our familar predictive distribution $p(y|f,X) \sim N(f, \sigma^2_nI)$, and represents how well $f$ maps $X$ to $y$. $p(f|X)$ is our prior distribution over weights $\sim N(0, K)$ which we use here to represent the complexity of $f$. Our weight's mean will always be the same under our prior, but our $K(X,X')$ tells us how "wiggly" our function is. The closer our $p(f|X)$ distribution is to the true complexity of the function, the higher our marginal likelihood. For example, for SE if our data is close together, then $|X - X'|$ becomes small and our covariance $k(X,X')$ on our function distribution prior is large. Therefore, we get a high variety of functions and a higher probability of sampling a more complex function. % Conversly,ew data $k(X_*, X_*')$ that is far away from each other and would require a more complex function, its density in this prior smooth distribution will be low, so we get a low $p(f|X_*)$. 
    
We can express $p(Y|X)$ as a Gaussian integral over the joint distribution of $f$ and $Y$ ($p(Y,f|X)$), and marginalise out $f$ to get this PDF:
\begin{equation} \label{eq:log_marginal_likelihood}
    log p(Y|X) = -\frac{1}{2}Y^T(K + \sigma^2_nI)^{-1}Y - \frac{1}{2}log|K + \sigma^2_nI| - \frac{n}{2}log(2\pi)
\end{equation}

Alternatively, from \ref{eq:linear_model} we know that $y$ is Gaussian. Since both $y$ and $f$ are Gaussian, we can simply add their means and variances: 
\begin{equation*}
    p(Y|X) = N(0, K + \sigma^2_nI)
\end{equation*}
We can plug these mean and variances into Gaussian PDF \ref{eq:gaussian_pdf} to get \ref{eq:log_marginal_likelihood}.


\subsubsection{Algorithm for predictive distribution}
% Here is a practical algorithm using the above equations to compute the predictive distribution of a Gaussian process regression model:
\begin{enumerate}
    \item Take in inputs $X$, outputs $y$, covariance function $k$, noise level $\sigma^2_n$, and test input $X_*$
    \item $L = \text{cholesky}(K(X,X) + \sigma_n^2I)$
    \begin{itemize}
        \item Invert our $[K(X,X) + \sigma^2_nI]$ matrix needed for mean and variance using Cholesky decomposition
    \end{itemize}
    \item $alpha = L^T \backslash (L \backslash y)$
    \begin{itemize}
        \item Prepare the mean of our predictive distribution in linear combination form by computing the $\alpha$ vector
    \end{itemize}
    \item $mu = K(X_*, X)^T \cdot alpha$
    \begin{itemize}
        \item Compute the mean
    \end{itemize}
    \item $v = L \backslash K(X_*, X)^T$
    \begin{itemize}
        \item Prepare to compute variance by computing $v$, the form in which $L$ is used in the variance
    \end{itemize}
    \item $var = K(X_*, X_*) - v^T v$
    \begin{itemize}
        \item Compute the variance
    \end{itemize}
    \item $\text{log} p(Y|X) = -\frac{1}{2}y^T \cdot alpha - \frac{1}{2}log|K(X,X) + \sigma^2_nI| - \frac{n}{2}log(2\pi)$
    \begin{itemize}
        \item Compute the log marginal likelihood 
    \end{itemize}
    \item Return the mean $mu$, variance $var$, and log marginal likelihood
\end{enumerate}

TODO Choelsky decomposition if needed, missing background


\subsection{Varying the hyperparameters \cite{gp-ml}}
Our covariance functions has some hyperparameters, e.g. the full form of SE in one dimension contains some free parameters $\sigma^2_f$, $\sigma^2_n$, and $l$:
\begin{equation*}
    k_y(x,x') = \sigma^2_f \exp\left(-\frac{1}{2}\frac{|x - x'|^2}{l^2}\right) + \sigma^2_n\delta_{x,x'}
\end{equation*}
Note that our covariance function is for $k_y$ as it is for the noisy targets $y$, not the function $f$. $\sigma^2_f$ is the signal variance, which controls the overall scale of the function. $\sigma^2_n$ is the noise variance, which controls the amount of noise in the observations. $\delta_{X,X'}$ is the Kronecker delta which represents our independence of noise assumption.

$l$ is a "length scale" hyperparameter that controls how sensitive our functions are - if we specify a lower $l$, we can "artificially" get a high $k(X,X')$. One way to determine $l$ is by the expected number of "upcrossings" that our kernel is expected to make for a given level $u$. A function performs an upcrossing for $u$ when $u = f(x)$ and $dy/dx > 0$. For example, with $u = 2$ and $y = x^2$, there exists one upcrossing at $(2, \sqrt{2})$ where $dy/dx = 4$, and a downcrossing at $(2, -\sqrt{2})$ where $dy/dx = -4$. For our zero-mean Gaussian processes, the expected number of upcrossings of our (stationary) kernel for a level $ 0 < u < 1$ is:
\begin{equation*}
    \mathbb{E}[N_u] = \frac{1}{2\pi} \sqrt{\frac{-k''(0)}{k(0)}} \exp \left(-\frac{u^2}{2k(0)}\right)
\end{equation*}
We can empirically count the number of upcrossings between 0 and 1 and set this equal to the expected number of upcrossings to get a value for $l$:
\begin{equation} \label{eq:general_l}
        \frac{1}{2\pi} \sqrt{\frac{-k''(0)}{k(0)}} \exp \left(-\frac{u^2}{2k(0)}\right) = \hat{N}_u
\end{equation}

A large amount of upcrossings implies our data-generating function wiggles rapidly, so our $l$ becomes smaller to produce a covariance function that in turn produces more flexible functions.

\subsection{Smoothing and equivelant kernels \cite{gp-ml}}
TODO, missing background

\subsubsection{Linear predictors and smoothers}
% The representer theorem states that we can represent a kernel in finite-dimensional space, even if properties of our method mean we have an infinite-dimensional feature space. For example, using SE we could obtain an infinite number of basis functions, so our data could be projected into an infinite dimensional space. Any other function than the mean drawn from our predictive distribution would have to be in infinite-dimension space. However, our mean is dependent on our $n$-length training labels $y$. Therefore, TODO.
% The mean of our predictive distribution can be represented as a "linear predictor", or as a linear combination of $X$ and our covariance function:
% \begin{equation} \label{eq:gp_linear_predictor}
%     \begin{aligned}
%         \alpha_i = K(X_i, X_i) + \sigma_n^2I)^{-1}f(X)_i \\
%         \mathbb{E}_{p(f(X_*)|X_*,X,Y)}[f(X_*)] = \sum_{i=1}^n \alpha_i k(X_*, X_i)
%     \end{aligned}
% \end{equation}
% Here, $\alpha_i$ is a "linear smoother" because it is a linear combination of $y$ and our covariance function.

Understanding how our predictive distribution's mean varies with its inputs is difficult because of the first term $[(K(X, X) + \sigma_n^2I)]^-1$: it is dependent on exact values of $X$, and it requires the inversion of $K(X, X) + \sigma^2_nI$. Instead, we can reformulate $E_{p(f(X_*)|X_*,X,Y)}[f(X_*)]$ as an "equivelant kernel" to remove the dependence on $X$. 
which uses a kernel smoother (also known as the Nadaraya-Watson estimator)  

Firstly, we rewrite our mean such that our mean function is a "linear smoother", or a function of the covariance function and our training data labels $Y$:
\begin{equation} \label{eq:gp_linear_smoother}
    \begin{aligned}
        H(X_*) = [K(X, X) + \sigma^2_nI]^{-1}K(X_*,X) \\
        \mathbb{E}_{y_*|X_*,X,y)}[f(X_*)] = H(X_*)^T Y
    \end{aligned}
\end{equation}
This vector of functions $H(X_*)$ is called a weight function and contains our problematic term. 

Instead of inputting the values of $x_i$ directly into the kernel function, we can use $x_i$ centred around $X_*$: 
\begin{equation*}
    k_i = k(|x_i - x_{*i}|/l)
\end{equation*}
Where $l$ is some length scale hyperparameter. 

TODO explain what we gain from this

Thus, our predictive distribution mean becomes:
\begin{equation*}
    \mathbb{E}_{p(f(X_*)|X_*,X,Y)}[f(X_*)] = \sum_{i=1}^n w_i y_i
\end{equation*}
where $w_i = k_i / \sum_{j=1}^n k_j$ is a TODO.


% \subsubsection{Explicit basis functions}

\end{document}
