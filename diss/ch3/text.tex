\section{Computational Issues}

Inverting the $[K(X,X) + \sigma^2_nI]$ matrix in our predictive distribution scales poorly with the number of training data points $n$, as inverting the $n \times n$ matrix $X$ that represents our training data is $O(n^3)$. There exist significantly more powerful approximations of this inversion for "celerite" kernels. Strategies for any covariance function fall into two categories: those that produce a single approximation for the entire dataset, or those that produce several approximations that are "experts" in a particular region of the dataset and combine these local approximations to form a global approximation. 

\subsection{Global approximations \cite{big-data}}

\subsubsection{Subset-of-data (SOD)}
As discussed previously, matrix approximations like the Nystrom approximation can produce kernels that violate PSD due to the irreduceable error involved. Instead of producing a low-rank approximation of the covariance matrix from a full GP, we could guarantee PSD by applying the GP to a subset $M$ of $X$ to reduce the cost of inversion to $O(m^3)$, where $m$ is the number of training points in $M$. A theoretical graphon analysis showed that choosing $M$ randomly gives an accuracy of $O(log^{-1/4}m)$ for the predictive mean and variance, which produces more accurate predictions with faster runtimes than sparse approximations as $n$ increases. \cite{random-subsampling} Subset-of-data also requires no analytic assumptions about the covariance functions.

% using a subset of $M$ that is representative of the entire dataset by clustering the data, e.g. using a k-means algorithm, and using these cluster centroids as our subset. 

We can reduce $m$ needed to achieve the same level of accuracy with a "greedy" approach by determining the gain in likelihood from including each data point $x_i$ in X, adding the maximum gain in likelihood point to $M$ and repeating until the size of $M$ reaches $m$. However, computational savings from reducing $m$ are smaller than the cost of searching $X$ for these centroids $O(n^2m)$. Instead, we can use a "matching pursuit" approach - maintain a cache of the already precomputed kernel values, and use these to compute the gain in likelihood for each point in $X$ in $O(nm^2)$ time. \cite{matching-pursuit}

\subsubsection{Sparse kernels}
A sparse kernel is a particularly designed kernel that imposes $k(X,X') = 0$ if $|X - X'|$ is larger than some threshold $d$ to create a sparse covariance matrix. This reduces the number of calculations that need to be performed and computational complexity to $O(an^3)$, where $a$ is the proportion of non-zero entries remaining, but the kernel needs to be carefully designed to work with zeroes and ensure all entries are PSD. TODO sparse RBF


\subsubsection{TODO Sparse approximations}
SOD throws away all data not in the selected subset of real training data $m$, losing information and reducing accuracy. Sparse approximation approaches are more accurate than SOD whilst achieving the same computational cost $O(nm^2 + m^3)$, by using potentially imaginary inducing points $X_m$ that best approximate the full covariance matrix $K(X,X)$. 

Joining a new prior on $f(X_m)$ to our previous prior on $f(X)$ \ref{eq:joint_prior} gives us a new joint prior:
\begin{equation*}
    \begin{pmatrix}
        f(X)
        f(X_m) \\
    \end{pmatrix} \sim \mathcal{N}\left(
    \begin{pmatrix}
        0 \\
        0 \\
    \end{pmatrix},
    \begin{pmatrix}
        K(X, X) & K(X, X_m) \\
        K(X_m, X) & K(X_m, X_m)    
    \end{pmatrix}
    \right)
\end{equation*}
Similarly to \ref{eq:conditioning}, we can use the Gaussian conditioning identity to condition this joint prior on our inducing points $f(X_m)$ and obtain a posterior distribution over $f(X)$:
\begin{equation} \label{eq:conditioning_approx}
    p(f(X) | f(X_m)) \sim \mathcal{N}\left(
    K(X, X_m) K(X_m, X_m)^{-1} f(X_m),
    K(X, X) - K(X, X_m) K(X_m, X_m)^{-1} K(X_m, X)
    \right)
\end{equation}
We can show this approach is equivelant to our original priors once we marginalise out $f(X_m)$:
\begin{equation*}
    p(f(X)) = \int p(f(X) | f(X_m)) p(f(X_m)) df(X_m)
\end{equation*}
It can be shown \cite{big-data} that this integral recovers our full priors on $f(X)$ \ref{jeq:joint_prior}:
\begin{equation*}
    p(f(X)) = \mathcal{N}\left(0, K(X, X)\right)
\end{equation*}
We can assemble our full joint prior \ref{eq:joint_prior} and posterior \ref{eq:conditioning} as before to produce our final predictive distribution.

We have introduced an additional first step into the assembly of the predictive distribution that determines our prior on $f(X)$ with no additional computational cost. By introducing approximations into this new step, we assemble $f(X)$ priors that are faster to work with than the full priors. These approximations are divided into two categories: prior approximations, which change $p(f(X) | f(X_m))$, and posterior approximations, which change how the final predictive disitribution is assembled given $p(f(X))$.

\paragraph{Prior approximations}

\subparagraph{Subset-of-Regression (SoR) \cite{sor}}
SoR sets the variance of \ref{eq:conditioning_approx} to zero:
\begin{equation*}
    p_{\text{SoR}}(f(X) | f(X_m)) \sim \mathcal{N}\left(
    K(X, X_m) K(X_m, X_m)^{-1} f(X_m),
    0
    \right)
\end{equation*}
Producing a marginal prior:
\begin{equation*}
    p_{\text{SoR}}(f(X)) = \int p_{\text{SoR}}(f(X) | f(X_m)) p(f(X_m)) df(X_m)
\end{equation*}
Evaluating this integral and representing the result as a Gaussian distribution:
\begin{equation*}
    p_{\text{SoR}}(f(X)) = \mathcal{N}\left(
    0, 
    K(X, X_m) K(X_m, X_m)^{-1} K(X_m, X)
    \right)
\end{equation*}
Because our new variance $K(X, X_m) K(X_m, X_m)^{-1} K(X_m, X)$ is rank $m$, we can use the Sherman-Morrison-Woodbury formula \ref{eq:A_phi_inverse_phi_X} to compute the inverse of the covariance matrix in our predictive distribution in $O(nm^2)$ time. However, setting our prior's variance to zero severely underestimates uncertainty, producing GPs that are too confident far from $X_m$.

\subparagraph{Fully independent training conditional (FITC)}

\paragraph{Posterior approximations}

\paragraph{Structured sparse approximation}

\paragraph{Selecting inducing points}



\subsection{TODO Local approximations \cite{big-data}}

\subsubsection{Naive-local-experts}

\subsubsection{Mixture-of-experts}

\subsubsection{Product-of-experts}


\subsection{TODO Approximations for Celerite kernels \cite{foreman-mackay}}


% \subsubsection{Extensions}
% 
% \paragraph{Scalable manifold GP}
% 
% \paragraph{Scalable deep GP}
% 
% \paragraph{Scalable online GP}
% 
% \paragraph{Scalable multi-task GP}
% 
% \paragraph{Scalable recurrent GP}
% 
% \paragraph{Scalable GP classification}
