\contentsline {section}{\numberline {1}Introduction}{4}{section.1}%
\contentsline {section}{\numberline {2}Defining the Gaussian Process}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}Weight-space view \blx@tocontentsinit {0}\cite {gp-ml}}{4}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Standard linear model}{4}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}Posterior distribution}{4}{subsubsection.2.1.2}%
\contentsline {paragraph}{\numberline {2.1.2.1}Deriving our posterior}{4}{paragraph.2.1.2.1}%
\contentsline {paragraph}{\numberline {2.1.2.2}Deriving the properties of the posterior by completing the square}{5}{paragraph.2.1.2.2}%
\contentsline {paragraph}{\numberline {2.1.2.3}Gaussian posteriors and ridge regression}{6}{paragraph.2.1.2.3}%
\contentsline {subsubsection}{\numberline {2.1.3}Predictive distribution}{6}{subsubsection.2.1.3}%
\contentsline {paragraph}{\numberline {2.1.3.1}Deriving the predictive distribution}{6}{paragraph.2.1.3.1}%
\contentsline {subsubsection}{\numberline {2.1.4}Projections of inputs into feature space}{7}{subsubsection.2.1.4}%
\contentsline {subsubsection}{\numberline {2.1.5}Computational issues}{8}{subsubsection.2.1.5}%
\contentsline {paragraph}{\numberline {2.1.5.1}Avoiding inversion of $A_{\phi }$}{8}{paragraph.2.1.5.1}%
\contentsline {paragraph}{\numberline {2.1.5.2}Kernels and the kernel trick}{8}{paragraph.2.1.5.2}%
\contentsline {subsection}{\numberline {2.2}Function-space view \blx@tocontentsinit {0}\cite {gp-ml}}{9}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Gaussian processes (GP)}{9}{subsubsection.2.2.1}%
\contentsline {paragraph}{\numberline {2.2.1.1}Definition}{9}{paragraph.2.2.1.1}%
\contentsline {paragraph}{\numberline {2.2.1.2}Consistency requirement}{9}{paragraph.2.2.1.2}%
\contentsline {paragraph}{\numberline {2.2.1.3}Bayesian linear regression as a GP}{9}{paragraph.2.2.1.3}%
\contentsline {paragraph}{\numberline {2.2.1.4}Function evaluations to a random function}{9}{paragraph.2.2.1.4}%
\contentsline {subsubsection}{\numberline {2.2.2}Predictive distributions with noise-free observations}{10}{subsubsection.2.2.2}%
\contentsline {paragraph}{\numberline {2.2.2.1}Prior distribution over functions}{10}{paragraph.2.2.2.1}%
\contentsline {paragraph}{\numberline {2.2.2.2}Posterior distribution over functions}{10}{paragraph.2.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.3}Predictive distributions with noisy observations}{10}{subsubsection.2.2.3}%
\contentsline {paragraph}{\numberline {2.2.3.1}Noisy observations prior}{10}{paragraph.2.2.3.1}%
\contentsline {paragraph}{\numberline {2.2.3.2}Noisy observations posterior}{10}{paragraph.2.2.3.2}%
\contentsline {subsubsection}{\numberline {2.2.4}Marginal likelihood}{10}{subsubsection.2.2.4}%
\contentsline {subsubsection}{\numberline {2.2.5}Algorithm for predictive distribution}{11}{subsubsection.2.2.5}%
\contentsline {subsection}{\numberline {2.3}Varying the hyperparameters \blx@tocontentsinit {0}\cite {gp-ml}}{11}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Smoothing and equivelant kernels \blx@tocontentsinit {0}\cite {gp-ml}}{12}{subsection.2.4}%
\contentsline {subsubsection}{\numberline {2.4.1}Linear predictors and smoothers}{12}{subsubsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.2}Explicit basis functions}{12}{subsubsection.2.4.2}%
\contentsline {section}{\numberline {3}Exploring Covariance Functions \blx@tocontentsinit {0}\cite {gp-ml}}{12}{section.3}%
\contentsline {subsection}{\numberline {3.1}Characteristics of covariance functions \blx@tocontentsinit {0}\cite {gp-ml}}{12}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Stationarity and isotropicism}{12}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Symmetry and positive semidefiniteness}{12}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Mean square continuity and differentiability}{12}{subsubsection.3.1.3}%
\contentsline {subsection}{\numberline {3.2}Stationary covariance functions \blx@tocontentsinit {0}\cite {gp-ml}}{13}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Spectral density for stationary processes}{13}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Squared exponential (SE)}{13}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Matern-class}{14}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Exponential and $\gamma $-exponential}{17}{subsubsection.3.2.4}%
\contentsline {subsubsection}{\numberline {3.2.5}Rational quadratic}{19}{subsubsection.3.2.5}%
\contentsline {subsection}{\numberline {3.3}Non-stationary covariance functions \blx@tocontentsinit {0}\cite {gp-ml}}{20}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Sum and product}{20}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Neural network}{20}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Warping and periodicity}{20}{subsubsection.3.3.3}%
\contentsline {subsection}{\numberline {3.4}Language-processing covariance functions \blx@tocontentsinit {0}\cite {gp-ml}}{20}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}String}{20}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}Fisher}{20}{subsubsection.3.4.2}%
\contentsline {subsection}{\numberline {3.5}Factor-processing covariance functions \blx@tocontentsinit {0}\cite {gaopro}}{20}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}Ordered factor}{20}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}Factor}{20}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}Gower factor}{20}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}Indices-ignoring}{20}{subsubsection.3.5.4}%
\contentsline {subsection}{\numberline {3.6}Deriving kernels \blx@tocontentsinit {0}\cite {deriving-kernels}}{20}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Learning best kernel from data \blx@tocontentsinit {0}\cite {choosing-kernels}}{20}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Additive covariance kernels for high-dimensional learning \blx@tocontentsinit {0}\cite {additive-kernels}}{20}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Hierarchical Bayesian covariance function for hierarchical modelling \blx@tocontentsinit {0}\cite {hierarchical-kernels}}{20}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Free-form covariance matrix for multi-task learning \blx@tocontentsinit {0}\cite {freeform-kernels}}{20}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Combining different kernels for multi-task learning \blx@tocontentsinit {0}\cite {multi-kernels}}{20}{subsection.3.11}%
\contentsline {section}{\numberline {4}Computational Issues}{20}{section.4}%
\contentsline {subsection}{\numberline {4.1}Matrix inversion \blx@tocontentsinit {0}\cite {big-data}}{20}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Global approximations}{20}{subsubsection.4.1.1}%
\contentsline {paragraph}{\numberline {4.1.1.1}Subset-of-data}{20}{paragraph.4.1.1.1}%
\contentsline {paragraph}{\numberline {4.1.1.2}Sparse kernels}{20}{paragraph.4.1.1.2}%
\contentsline {paragraph}{\numberline {4.1.1.3}Sparse approximations}{20}{paragraph.4.1.1.3}%
\contentsline {subparagraph}{\numberline {4.1.1.3.1}Prior approximation}{20}{subparagraph.4.1.1.3.1}%
\contentsline {subparagraph}{\numberline {4.1.1.3.2}Posterior approximation}{20}{subparagraph.4.1.1.3.2}%
\contentsline {subparagraph}{\numberline {4.1.1.3.3}Structured sparse approximation}{21}{subparagraph.4.1.1.3.3}%
\contentsline {subsubsection}{\numberline {4.1.2}Local approximations}{21}{subsubsection.4.1.2}%
\contentsline {paragraph}{\numberline {4.1.2.1}Naive-local-experts}{21}{paragraph.4.1.2.1}%
\contentsline {paragraph}{\numberline {4.1.2.2}Mixture-of-experts}{21}{paragraph.4.1.2.2}%
\contentsline {paragraph}{\numberline {4.1.2.3}Product-of-experts}{21}{paragraph.4.1.2.3}%
\contentsline {subsubsection}{\numberline {4.1.3}Improvements}{21}{subsubsection.4.1.3}%
\contentsline {paragraph}{\numberline {4.1.3.1}Scalability}{21}{paragraph.4.1.3.1}%
\contentsline {paragraph}{\numberline {4.1.3.2}Capability}{21}{paragraph.4.1.3.2}%
\contentsline {subsubsection}{\numberline {4.1.4}Extensions}{21}{subsubsection.4.1.4}%
\contentsline {paragraph}{\numberline {4.1.4.1}Scalable manifold GP}{21}{paragraph.4.1.4.1}%
\contentsline {paragraph}{\numberline {4.1.4.2}Scalable deep GP}{21}{paragraph.4.1.4.2}%
\contentsline {paragraph}{\numberline {4.1.4.3}Scalable online GP}{21}{paragraph.4.1.4.3}%
\contentsline {paragraph}{\numberline {4.1.4.4}Scalable multi-task GP}{21}{paragraph.4.1.4.4}%
\contentsline {paragraph}{\numberline {4.1.4.5}Scalable recurrent GP}{21}{paragraph.4.1.4.5}%
\contentsline {paragraph}{\numberline {4.1.4.6}Scalable GP classification}{21}{paragraph.4.1.4.6}%
\contentsline {section}{\numberline {5}Applying a Gaussian process to TBD}{21}{section.5}%
\contentsline {subsection}{\numberline {5.1}Materials science \blx@tocontentsinit {0}\cite {materials}}{21}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Cosmography \blx@tocontentsinit {0}\cite {cosmography}}{21}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Statistical emulators \blx@tocontentsinit {0}\cite {emulators}}{21}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Signals processing \blx@tocontentsinit {0}\cite {signals-processing}}{21}{subsection.5.4}%
\contentsline {section}{\numberline {6}Conclusion}{21}{section.6}%
