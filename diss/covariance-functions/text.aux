\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {3}Exploring Covariance Functions}{13}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Characteristics of covariance functions and matrices}{13}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Gram matrices}{13}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Eigenvalue and eigenfunctions of covariance matrices}{13}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Integral operators}{13}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mercer's theorem}{14}{subsubsection.3.1.2}\protected@file@percent }
\newlabel{eq:gp_mercer}{{25}{14}{Mercer's theorem}{equation.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Choosing the length scale and other hyperparameters}{14}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Analysing the length scale}{14}{subsubsection.3.1.3}\protected@file@percent }
\newlabel{eq:taylor_l}{{26}{14}{Analysing the length scale}{equation.26}{}}
\@writefile{toc}{\contentsline {paragraph}{Length scale and upcrossings}{15}{equation.26}\protected@file@percent }
\newlabel{eq:upcrossings}{{27}{15}{Length scale and upcrossings}{equation.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Mean square continuity and differentiability}{15}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean square space}{15}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MS continuity}{16}{subsubsection.3.1.4}\protected@file@percent }
\newlabel{eq:ms_continuity}{{28}{16}{MS continuity}{equation.28}{}}
\@writefile{toc}{\contentsline {paragraph}{MS differentiability}{16}{equation.28}\protected@file@percent }
\newlabel{eq:msd_a}{{29}{16}{MS differentiability}{equation.29}{}}
\newlabel{eq:msd_a_final}{{30}{17}{MS differentiability}{equation.30}{}}
\newlabel{eq:msd_b_final}{{31}{18}{MS differentiability}{equation.31}{}}
\newlabel{eq:msd_ab_final}{{32}{19}{MS differentiability}{equation.32}{}}
\newlabel{eq:ms_differentiability}{{33}{19}{MS differentiability}{equation.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stationary covariance functions}{19}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Stationarity and isotropicism}{19}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Spectral density}{19}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{eq:bochner}{{34}{19}{Spectral density}{equation.34}{}}
\newlabel{eq:spectral-density}{{35}{19}{Spectral density}{equation.35}{}}
\newlabel{eq:weiner-khinchin}{{36}{20}{Spectral density}{equation.36}{}}
\@writefile{toc}{\contentsline {paragraph}{Spectral density and MS differentiability}{20}{equation.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}GPs from stationary covariance functions in MS space}{20}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Stationary covariance functions}{20}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Squared exponential (SE)}{20}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{eq:se}{{37}{20}{Squared exponential (SE)}{equation.37}{}}
\newlabel{eq:se-sd}{{38}{20}{Squared exponential (SE)}{equation.38}{}}
\@writefile{toc}{\contentsline {paragraph}{From feature space to SE}{21}{equation.38}\protected@file@percent }
\newlabel{eq:feature-space-se}{{39}{21}{From feature space to SE}{equation.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Plot of a GP using SE applied to a smooth sin wave $y = \sin (2 * pi * x) + \epsilon $, $\epsilon \sim N(0, 0.05)$, fitted using the R package GauPro \blx@tocontentsinit {0}\cite {gaupro}. \\ The noiseless sin-wave is in blue and some sample datapoints taken from this function with noise are in black. The black line represents the expected function from the Gaussian process. The green line represents the 95\% confidence interval around the predictive distribution without the $\sigma ^2_n$ term, representing the uncertainty surrounding predictions of the noise-free mean function $f(X)$. The red line represents the 95\% confidence interval with $\sigma ^2_n$, representing the uncertainty surrounding predictions of the noisy observations $y$. \\ For this extremely smooth function with low noise, SE nearly perfectly captures the original data-generating function. The variances fall as they approach datapoints and rise as they move away from them. Defining our point of interest as $x_*$, as $x_*$ approaches a training datapoint $x$, $x - x*$ approaches zero and the value of our SE kernel approaches one, and the predictive distribution at $x_*$ collapses close to our training data $N(Y \sigma _n^2I, \sigma _n^2I)$. This is known as the interpolation property \blx@tocontentsinit {0}\cite {gp-ml} and is valid for all stationary covariance functions we cover. }}{22}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Plots of some sample functions drawn from a GP trained on the smooth dataset using SE. The blue and black lines are, as before, the noiseless data-generating function and the mean function drawn from the GP respectively. The green lines are 20 functions drawn from the GP - if we superimposed these draws onto the previous GP of means and variances, we would expect approximately 95\% of these function draws to be contained within the red variance lines. \\ Although we can technically draw infinite sample functions from any GP, these function draws all look very similar. This is because very few functions exist that satisfy SE's infinite smoothness and can exist entirely within this variance envelope, and they inevitably share many properties. \\ These constraints also affect the sample functions' ability to obey the interpolation property and require a trade-off between smoothness and fidelity. For example, there are no sample functions that pass through the fourth point at $(0.21, 0.86)$ because infinitely smooth functions are too rigid to pass through this point and the next two points at $(0.29, 1)$ and $(0.36, 0.81)$. This violation of the interpolation property is a consequence of the noise term $\epsilon \sim N(0, 0.05)$ in the data-generating function. If the noise term was zero and the data-generating function was infinitely differentiable, like this one, then the GP and all sample paths would pass through all datapoints. }}{23}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plot of the expected function draw and variances as before, using SE fitted to a rougher function: $\begin {cases} x < 0.3 & x \\ 0.3 \leq x < 0.7 & 1 - x \\ x \geq 0.7 & x - 0.5 \end  {cases}$\\ SE performed worse on this rougher data as its extreme smoothness is unable to adapt well. The length scale has decreased (0.55, compared to 0.76 using smooth data), which reduces the rate of variance decay with frequency \ref {eq:se-sd} and increases the oscillatory behaviour of the GP sample functions whilst still producing infinitely smooth functions. }}{24}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of some sample functions drawn from a GP as before, trained on the rough dataset using SE. \\ These functions are much more varied than those drawn from the smooth dataset, as SE is unable to adapt to the rougher data. Some draws have become more oscillatory due to the decrease in length scale, but all draws are still infinitely smooth. }}{24}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Rational quadratic (RQ)}{25}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Plot of the expected function draw and variances as before, using RQ fitted to the smooth sin wave dataset. \\ The GauPro package \blx@tocontentsinit {0}\cite {gaupro} in R used to fit these GPs has chosen an extremely high $\alpha = 100$ via MLE because of the smoothness of the data-generating function. This high $\alpha $ means we are very likely to choose one length scale $l$ and thus RQ is nearly indistinguishable from SE. }}{26}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Plot of the expected function draw and variances as before, using RQ fitted to the rougher dataset. \\ The rougher dataset has driven $\alpha $ to be much lower than in the smooth dataset, at $\alpha = 1.04$, and RQ becomes a true heavy-tailed mixture distribution of SEs over length scales. \\ The green variance lines disappear because the expected sample path passes through all datapoints, so any new training point is expected to be exactly equal to the expected function draw at that point, and the variance is zero. If the data-generating function became too rough or we used a more inflexible kernel like SE, we would need these sample variance lines to reflect how far from the expected function draw we expect the sample functions to be. \\ }}{27}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Plots of some sample functions drawn from a GP as before, trained on the rough dataset using RQ. \\ Each function draw expresses a different combination of high and low length scale components drawn from this fixed inverse-gamma distribution, and exhibit different behaviours at different frequencies. This is especially visible as the difference between function draws widens between data points - some function draws oscillate less than others as they are capturing a longer-term, low-frequency trend in the data. These more flexible functions are enough for the expected function draw to pass through all datapoints and satisfy the interpolation property. }}{27}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}$\gamma $-exponential and exponential}{28}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Plot of the expected function draw and variances as before, using $\gamma $-exponential fitted to the smooth sin wave dataset. \\ The GauPro package \blx@tocontentsinit {0}\cite {gaupro} in R has chosen $\gamma = 2$ via MLE, which produces SE. This is because the smoothness of the data-generating function is so high that no other covariance function can capture it. }}{28}{figure.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Plot of the expected function draw and variances as before, using $\gamma $-exponential fitted to the rougher dataset. \\ $\gamma = 1.51$ has been selected by GauPro \blx@tocontentsinit {0}\cite {gaupro} via MLE, which produces a covariance function that is not differentiable at $|x - x'| = 0$. This is because the roughness of the data-generating function is too high for SE to capture, and the $\gamma $-exponential covariance function is able to adapt to this roughness. \\ These variances appear not to collapse to zero at the training points. In fact, this is a graphical issue caused by the very rough functions that the $\gamma $-exponential covariance function produces. Unlike the functions drawn from an RQ-powered GP that remain close to the training points for some time before and after the training points, functions drawn from this distribution only remain close to the training points for a very short distance before diverging - a distance that is too small to be visible in this plot. }}{29}{figure.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Plots of some sample functions drawn from a GP as before, trained on the rough dataset using $\gamma $-exponential. \\ These function draws are much rougher than those drawn from RQ or SE thanks to their non-differentiability. }}{29}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Exponential covariance function}{29}{figure.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Plot of the expected function draw and variances as before, using exponential fitted to the smooth sin wave dataset. \\ The expected function draw is practically a straight line connecting each data point, as the exponential covariance function is not smooth enough to capture the sin wave. The graphical issue observed in $\gamma $-exponential is more pronounced here, with the variances barely approaching zero as $x_*$ approach a training point. }}{30}{figure.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  Plots of some sample functions drawn from a GP as before, trained on the smooth dataset using exponential. \\ These function draws are effectively white noise that collapses at training points. }}{31}{figure.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Plot of the expected function draw and variances as before, using exponential fitted to the rougher dataset. \\ The behaviour of GPs using the exponential covariance function is invariant to the data it is trained on - as with the smooth dataset, the expected function draw is a straight line connecting each data point, and the variances barely approach zero as $x_*$ approaches a training point. }}{31}{figure.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Matern-class}{31}{subsubsection.3.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Matern 3/2}{32}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{eq:matern-32}{{40}{32}{Matern 3/2}{equation.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  Plot of the expected function draw and variances as before, using Matern 3/2 fitted to the smooth sin wave dataset. \\ This GP's expected function draw goes directly to the training points, similar to the behaviour of the exponential covariance function. Unlike the exponential covariance function, the one-time MS differentiability constraint on the sample functions restricts the sample functions from resembling white noise, which produces narrower variances closer to the expected draw. }}{32}{figure.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  Plots of some sample functions drawn from a GP as before, trained on the smooth dataset using Matern 3/2. \\ These function draws are much smoother than those drawn from the exponential covariance function. }}{33}{figure.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Plots of some sample functions drawn from a GP as before, trained on the rough dataset using Matern 3/2. \\ Although these function draws are much smoother than those produced by the exponential covariance function, they are still rough between points. }}{33}{figure.16}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Matern 5/2}{33}{figure.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Plot of the expected function draw and variances as before, using Matern 5/2 fitted to the smooth sin wave dataset. \\ Here, Matern 5/2's expected function is smooth enough to resembles SE - a smooth function that did not pass through all training points. The variance of the noise-free function evaluations and the noisy observations are narrower than SE's, as the additional roughness allows the expected function draw to pass closer to the training points. For example, the previously examined point at $(0.21, 0.86)$ is now much closer to the expected function draw than under SE. }}{34}{figure.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Plot of the expected function draw and variances as before, using Matern 5/2 fitted to the rougher dataset. \\ Matern 5/2 has adapted well to the rough dataset - the expected function draw is smoother than the expected function draw from Matern 3/2, but remains rough enough to pass through all training points. The variances are the narrowest of all the covariance functions because the smoothness constraint is severe enough to prevent exponential-style white noise, but the roughness is high enough to approach the training points closely. }}{34}{figure.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  Plots of some sample functions drawn from a GP as before, trained on the rough dataset using Matern 5/2. \\ These function draws are much smoother than those drawn from Matern 3/2, particularly between data points, whilst still passing through all datapoints. }}{35}{figure.19}\protected@file@percent }
\@setckpt{covariance-functions/text}{
\setcounter{page}{36}
\setcounter{equation}{40}
\setcounter{enumi}{8}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{4}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{19}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{0}
\setcounter{Item}{8}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{29}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{33}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{float@type}{4}
}
