\section{Computational Issues}

Inverting the $[K(X,X) + \sigma^2_nI]$ matrix in our predictive distribution scales poorly with the number of training data points $n$, as inverting the $n \times n$ matrix $X$ that represents our training data is $O(n^3)$. There exist significantly more powerful approximations of this inversion for "celerite" kernels. Strategies for any covariance function fall into two categories: those that produce a single approximation for the entire dataset, or those that produce several approximations that are "experts" in a particular region of the dataset and combine these local approximations to form a global approximation.

TODO why i didnt investigate local approximations 

% \subsection{Global approximations}



\subsection{Cholesky decomposition}
TODO why not sparse kernels?

Cholesky decomposition \cite{gp-ml} is a matrix factorisation technique specialised for Gram matrices. Analagous to taking the square root of a Gram matrix, Cholesky finds a unique lower-traingular matrix $L$ with positive diagonal entries for some Gram matrix $A$ such that:
\begin{equation} \label{eq:cholesky-factor}
    A = LL^T
\end{equation}
For example, let:
\begin{equation*}
    A = \begin{pmatrix}
        4 & 12 & -16 \\ 
        12 & 37 & -43 \\
        -16 & -43 & 98
    \end{pmatrix}
\end{equation*}
L would be:
\begin{equation*}
    L = \begin{pmatrix}
        2 & 0 & 0 \\
        6 & 1 & 0 \\
        -8 & 5 & 3 
    \end{pmatrix}
\end{equation*}
such that:
\begin{equation*}
    LL^T = \begin{pmatrix}
        2 & 0 & 0 \\
        6 & 1 & 0 \\
        -8 & 5 & 3 
    \end{pmatrix} \begin{pmatrix}
        2 & 6 & -8 \\
        0 & 1 & 5 \\
        0 & 0 & 3 
    \end{pmatrix} = \begin{pmatrix}
        4 & 12 & -16 \\ 
        12 & 37 & -43 \\
        -16 & -43 & 98
    \end{pmatrix}
\end{equation*}

We can assemble $L$ using the Cholesky algorithm. \cite{big-data} For diagonal entries in A, this costs $O(n)$:
\begin{equation*}
    L_{i,i} = \sqrt{A_{i,i} - \sum_{k=1}^{i-1} L_{i,k}^2}
\end{equation*}
For off-diagonal entries:
\begin{equation*}
    L_{i,j} = \begin{cases}
        \frac{1}{L_{j,j}} \left( A_{i,j} - \sum_{k=1}^{j-1} L_{i,k} L_{j,k} \right) & \text{if } i > j \\
        0 & \text{if } i < j
    \end{cases}
\end{equation*}
The computational cost of obtaining the Cholesky factorisation is $O(\frac{1}{3} n^3)$ thanks to the $\sum$ terms in the off-diagonal entries where $i > j$.

Once we have $L$, we can invert $A$ by solving the two triangular systems:
\begin{equation*}
    A^{-1} = L^{-T} L^{-1}
\end{equation*}
$L^{-1}$ comes from solving the forward substitution problem $LY = I$, where $I$ is the identity matrix, and $L^{-T}$ comes from solving the backward substitution problem $L^TX = Y$. Inverting $L$ costs $O(n^3)$, for a total complexity of $O(n^3)$ for Cholesky factorisation and inversion. 

The inversions of $K(X, X)$ that we need to form \ref{eq:conditioning} and \ref{eq:log_marginal_likelihood} exist as products of other terms ($K(X,X)^{-1} Y$ or $K(X,X)^{-1} K(X, X_*)$). We can obtain these solutions by solving these traingular systems:
\begin{equation*}
    K(X, X)^{-1} Y = L^{-1} Y
\end{equation*}
$Y$ is the product $K(X, X)^{-1}$. 

Cholesky is the preferred method of handling the inversion of covariance matrices because of its extreme numerical stability (thanks to the positive diagonal entries), and its guarantee to produce a valid Gram matrix. Unfortunately, matrix inversion by Cholesky factorisation is prohibitively expensive thanks to the cubic complexity - a kernel with $n = 1000$ would require over two billion operations to invert. 



\subsection{Sparse kernels}
We can improve on the cubic complexity of Cholesky decomposition by using sparse kernels, which are designed to produce sparse covariance matrices that can be inverted more efficiently.

A sparse kernel \cite{big-data} is a particularly designed kernel that imposes $k(X,X') = 0$ if $|X - X'|$ is larger than some threshold $d$ to create a sparse covariance matrix. This reduces the number of calculations that need to be performed and computational complexity to $O(an^3)$, where $a$ is the proportion of non-zero entries remaining, but the kernel needs to be carefully designed to work with zeroes and ensure all entries are PSD. TODO sparse RBF


\subsection{Eigenfunction and eigenvalue approximation of covariance matrices \cite{gp-ml}}
TODO why not sparse kernels? Mercer's representation of the kernel \ref{eq:gp_mercer} provides a way of approximating any arbitrary Gram matrix. Instead of summing all the eigenfunctions from 1 to $\infty$, we can approximate our kernel to any degree of accuracy $M < N$ by restricting our sum to the largest $M$ eigenvalues:
\begin{equation} \label{eq:gp_mercer_approx}
    K(X, X') \approx \sum_{i=1}^{M} \lambda_i \phi_i(X) \phi_i(X')
\end{equation}
and invert this smaller matrix at a lower computational cost $O(M^3)$. However, obtaining the eigenfunctions and eigenvalues of an $N$-size covariance matrix matrix is $O(N^3)$ for a total cost of $O(N^3) + O(M^3)$. We can reduce the cost of obtaining these eigenfunctions and eigenvalues by solving our eigenproblem on a smaller random sample of $X$. These approximations, and subsequent approximations, come with irreduceable error which could create a kernel that no longer satisfies the Gram matrix constraint of positive semidefiniteness, violating Mercer's theorem and making them invalid covariance matrices. 

Instead of our integral weighing our kernel and eigenfunction with the measure $\mu$ as before, they become weighed in terms of the PDF of the training data $p(x)$:
\begin{equation*}
    \lambda_i \phi_i(x') = \int K(x, x') p(x) \phi_i(x) dx
\end{equation*}
We can approximate this continuous representation and make it discrete by representing our selected samples as a vector $X_l$ of size $l$:
\begin{equation*}
    \lambda_i \phi_i(X') \approx \frac{1}{l} \sum_{j=1}^{l} K(x_j, X') \phi_i(x_j)
\end{equation*}
Defining a vector $\Phi_i$ of all eigenfunction evaluations $\phi_i(x_j)$ on the $X_l$ sample:
\begin{equation*}
    \lambda_i \phi_i(X') \frac{1}{l} K(X_j, X') \Phi_i
\end{equation*}
Multiplying both sides by $l$:
\begin{equation*}
    n \lambda_i \phi_i(X') = K(X_j, X') \Phi_i
\end{equation*}
Setting $\lambda_i^{mat} = l \lambda_i$:
\begin{equation*}
    K(X_l, X') \phi_i(X') = \lambda_i^{mat} \Phi_i
\end{equation*}
This arrangement requires our covariance matrix to be evaluated with some $X'$. Without creating another sample or defeating the point of this approximation by using the whole of $X$, the only candidate for $X'$ is the $X_l$ sample. Setting $X' = X_l$, our $\phi_i(X')$ becomes the definition of $\Phi_i$:
\begin{equation*}
    K(X_l, X_l) \Phi_i = \lambda_i^{mat} \Phi_i
\end{equation*}
Here, $\Phi_i$ also plays the role of the eigenvector $U_i$ of $K(X_l, X_l)$, and $\lambda_i^{mat}$ plays the eigenvalue of $K(X_l, X_l)$. 

Although we introduced $\lambda_i^{mat} = l \lambda_i$, $\lambda_i^{mat}$ are the eigenvalues of the eigenproblem produced by sampling, whereas $\lambda_i$ are the eigenvalues from the original eigenproblem. Thus, it acts as an approximation:
\begin{equation*}
    \lim_{l \to \infty} \lambda_i = \frac{1}{l} \lambda_i^{mat} 
\end{equation*}

\subsubsection{Approximating $\Phi_i$ and $U_i$ with Nystrom}
Solving this eigenproblem is $O(l^3)$ - an improvement on the $O(n^3)$ cost of solving the previous eigenproblem, and the resulting $O(l^3) + O(M^3)$ could be lower than the naive $O(n^3)$ inversion cost (if we use sufficiently low degrees of accuracy $l$ and $M$). We can improve this further by approximating $\Phi_i$.

$\Phi_i$ and $U_i$ are not exactly equivelant thanks to differing normalisations. $\Phi_i$'s normalisation comes from the original Monte Carlo approximation: $\frac{1}{l} \sum_{j=1}^{l} K(x_j, X') \phi_i(x_j) \approx 1$, thus the sum and our $||\Phi_i||_2^2 \approx l$ and $||\Phi_i|| \approx \sqrt{l}$. $U_i$'s normalisation comes from solving the eigenproblem, which returns $||U_i||_2^2 = 1$ and $||U_i|| = 1$. We can convert between the two by scaling $\Phi_i$ by $\sqrt{l}$ to produce $U_i$.

The NystrÃ¶m method \cite{nystrom} extends this approximation from the sample points $X_l$ to the full set of points $X$:
\begin{equation*}
    \phi_i(X) \approx \frac{1}{\sqrt{l}} \sqrt{n} K(X_l, X) \Phi_i
\end{equation*}
This approximation further reduces the cost of solving the eigenproblem and inverting the low-rank matrix to $O(l^2 n) + O(M^3)$.


\subsection{Subset-of-data (SoD) \cite{big-data}}
Matrix approximations like the Nystrom approximation can produce kernels that violate PSD due to the irreduceable error involved. Instead of producing a low-rank approximation of the covariance matrix from a full GP, we could guarantee PSD by applying the GP to a subset $M$ of $X$ to reduce the cost of inversion to $O(m^3)$, where $m$ is the number of training points in $M$. A theoretical graphon analysis showed that choosing $M$ randomly gives an accuracy of $O(log^{-1/4}m)$ for the predictive mean and variance, which produces more accurate predictions with faster runtimes than sparse approximations as $n$ increases. \cite{random-subsampling} Subset-of-data also requires no analytic assumptions about the covariance functions.

% using a subset of $M$ that is representative of the entire dataset by clustering the data, e.g. using a k-means algorithm, and using these cluster centroids as our subset. 

We can reduce $m$ needed to achieve the same level of accuracy with a "greedy" approach by determining the gain in likelihood from including each data point $x_i$ in X, adding the maximum gain in likelihood point to $M$ and repeating until the size of $M$ reaches $m$. However, computational savings from reducing $m$ are smaller than the cost of searching $X$ for these centroids $O(n^2m)$. Instead, we can use a "matching pursuit" approach - maintain a cache of the already precomputed kernel values, and use these to compute the gain in likelihood for each point in $X$ in $O(nm^2)$ time. \cite{matching-pursuit}


\subsection{Sparse approximations \cite{big-data}}
SoD throws away all data not in the selected subset of real training data $m$, losing information and reducing accuracy. Sparse approximation approaches are more accurate than SoD whilst achieving the same computational cost $O(nm^2 + m^3)$, by using potentially imaginary inducing points $X_m$ that best approximate the full covariance matrix $K(X,X)$. 

Joining a new prior on $f(X_m)$ to our previous prior on $f(X)$ \ref{eq:joint_prior} gives us a new joint prior:
\begin{equation*}
    \begin{pmatrix}
        f(X)
        f(X_m) \\
    \end{pmatrix} \sim \mathcal{N} (
    \begin{pmatrix}
        0 \\
        0 \\
    \end{pmatrix},
    \begin{pmatrix}
        K(X, X) & K(X, X_m) \\
        K(X_m, X) & K(X_m, X_m)    
    \end{pmatrix}
    )
\end{equation*}
Similarly to \ref{eq:conditioning}, we can use the Gaussian conditioning identity to condition this joint prior on our inducing points $f(X_m)$ and obtain a posterior distribution over $f(X)$:
\begin{equation} \label{eq:conditioning_approx}
    \begin{aligned}
        p(f(X) | X, X_m, f(X_m)) \sim \mathcal{N} ( \\
        K(X, X_m) K(X_m, X_m)^{-1} f(X_m), \\
        K(X, X) - K(X, X_m) K(X_m, X_m)^{-1} K(X_m, X) \\
        )
    \end{aligned}
\end{equation}
Marginalise out $f(X_m)$:
\begin{equation*}
    p(f(X)) = \int p(f(X) | f(X_m)) p(f(X_m)) df(X_m)
\end{equation*}
Without any approximations, it can be shown \cite{big-data} that this integral recovers our full priors on $f(X)$ \ref{jeq:joint_prior}:
\begin{equation*}
    p(f(X)) = \mathcal{N}\left(0, K(X, X)\right)
\end{equation*}
We can also assemble our predictive distribution by forming a new joint prior and a posterior with the testing data $X_*$ by substituting $X$ for $X_*$:
\begin{equation*}
    \begin{aligned}
        p(f(X_*) | X_*, X, f(X_m)) \sim \mathcal{N} (
        K(X_*, X_m) K(X_m, X_m)^{-1} f(X_m),
        K(X_*, X_*) - K(X_*, X_m) K(X_m, X_m)^{-1} K(X_m, X_*)
        ) \\
    \end{aligned}
\end{equation*}
As with training data, marginalising out $X_m$ yields our original full prior on the training data $f(X_*) \sim \mathcal{N}(0, K(X_*, X_*))$.

We have introduced a new first step into the assembly of the predictive distribution that reduces the cost of prediction from $O(n^3)$ to $O(mn^2 + m^3$), but the cost of assembling the marginal likelihood remains $O(n^3)$ in training. Approximations aimed at reducing the complexity of computing our marginal likelihoods are divided into two categories: prior approximations which change $p(f(X) | f(X_m))$, and posterior approximations which change $p(f(X), f(X_m) | y)$.

\subsubsection{Prior approximations}

\paragraph{Subset-of-Regression (SoR) \cite{sor}}
SoR sets the variance of \ref{eq:conditioning_approx} to zero:
\begin{equation*}
    p_{\text{SoR}}(f(X_*) | f(X_m)) \sim \mathcal{N} (
    K(X_*, X_m) K(X_m, X_m)^{-1} f(X_m),
    0
    )
\end{equation*}
Producing a marginal prior:
\begin{equation*}
    p_{\text{SoR}}(f(X_*)) = \int p_{\text{SoR}}(f(X_*) | f(X_m)) p(f(X_m)) df(X_m)
\end{equation*}
Evaluating this integral and representing the result as a Gaussian distribution:
\begin{equation*}
    p_{\text{SoR}}(f(X_*)) = \mathcal{N}(
    0, 
    K(X_*, X_m) K(X_m, X_m)^{-1} K(X_m, X_*)
    )
\end{equation*}

Inverting the $m$-rank $K(X_m, X_M)^{-1}$ costs $O(m^3)$, and multiplying it by the $n$-rank $K(X_*, X_m)$ and $K(X_m, X_*)$ costs $O(n^2m)$, sped up using SMW \ref{eq:A_phi_inverse_phi_X} to yield $O(nm^2)$, for a predictive distribution complexity of $O(nm^2 + m^3)$. The resulting likelihood:
\begin{equation*}
    \begin{aligned}
        p_{\text{SoR}}(Y) = -\frac{1}{2} \log(2 \pi \sigma_n^2) \\
        - \frac{1}{2\sigma_n^2} Y ^T Y \\
        + \frac{1}{2\sigma_n^4} Y^T K(X, X_m) A^{-1} K(X_m, X) Y \\ 
        - \frac{1}{2} \log | A | \\
        + \frac{1}{2} \log | K(X_m, X_m) |
    \end{aligned}
\end{equation*}
The only inversion here is the rank-$m$ $A = K(X_m, X_m) + \sigma_n^{-2} K(X_m, X) K(X, X_m)$ matrix, which costs $O(m^3)$. This achieves the same computational cost as the Nystrom approximation but does so in a probabilistic framework that guarantees PSD. However, setting our prior's variance to zero produces a predictive distribution that severely underestimates uncertainty, producing GPs that are too confident far from $X_m$.

\paragraph{Fully independent training conditional (FITC) \cite{fitc}}
FITC assumes that the new data $f(X_*)$ is independent given the inducing points $f(X_m)$. Formally:
\begin{equation*}
    p(f(X_*) | f(X_m)) = \prod_{i=1}^n p(f(x_{i*}) | f(X_m))
\end{equation*}
TODO more motivation on prior yields. Our new prior distribution is:
\begin{equation*}
    p_{\text{FITC}}(f(X_*) | f(X_m)) = \mathcal{N}(
    f(X_*) | K(x_i, X_m) K(X_m, X_m)^{-1} f(X_m),
    \text{diag}[V_{nn}] 
    )
\end{equation*}
$V_{nn} = K(X_*, X_*) - K(X_*, X_m) K(X_m, X_m)^{-1} K(X_m, X_*)$. Our new predictive distribution becomes:
\begin{equation*}
    \begin{aligned}
        p_{\text{FITC}}(f(X_*)) \sim \mathcal{N}( \\
        k(X_*, X_m) \lambda^{-1} K(X_m, X_m) f(X_m), \\
        K(X_*, X_*) - K(X_*, X_*) - K(X_*, X_m) K(X_m, X_m)^{-1} K(X_m, X_*) + K(X_*, X_m) \lambda^{-1} K(X_m, X_*) \\
        )
    \end{aligned}
\end{equation*}
$\lambda = K(X_m, X_m) + K(X_m, X_*) \text{diag}[V_{nn}]^{-1} K(X_*, X_m)$. 

There are three matrices being assembled here: the at-most rank $n$ diag[$V$], the rank-$m$ $\lambda$ and the final predictive variance of rank $m$. $V$ is similar to SoR and costs $O(mn^2)$, whilst getting its diagonal costs $O(nm)$, and assembling $\lambda$ and the predictive variance is also $O(nm^2)$. We need to invert $\lambda$, $K(X_m, X_m)$ and $\text{diag}[V_{nn}]$ - the first two are rank $M$, and inverting a diagonal only costs $O(n)$. These operations reduce to an overall inference complexity of $O(nm^2 + m^3)$, and the resulting likelihood:
\begin{equation*}
    \begin{aligned}
        p_{\text{FITC}}(Y) = -\frac{n}{2} \log(2 \pi) \\
        - \frac{1}{2} \log | \lambda + \sigma_n^2 I_n | \\
        - \frac{1}{2} \log | K(X_m, X_m) + K(X_m, X) [\lambda + \sigma_n^2 I_n]^{-1} K(X, X_m) | \\
        + \frac{1}{2} \log | K(X_m, X_m) | \\
        - \frac{1}{2} y^T [\lambda + \sigma_n^2 I_n]^{-1} y \\
        + \frac{1}{2} y^T [\lambda + \sigma_n^2 I_n]^{-1} K(X, X_m) [K(X_m, X_m) + K(X_m, X) [\lambda + \sigma_n^2 I_n]^{-1} K(X_m, X) [\lambda + \sigma_n^2 I_n]^{-1} y
    \end{aligned}
\end{equation*}
requires an $O(m^3)$ inversion of $K(X_m, X_m) + K(X_m, X) [\lambda + \sigma_n^2 I_n]^{-1} K(X_m, X)$ and an $O(nm^2)$ inversion of $\lambda + \sigma_n^2 I_n$, for an overall training complexity of $O(nm^2 + m^3)$.

FITC requires the same training and inference complexity as SoR but achieves a more accurate approximation for variance. Additionally, Bauer et. al. \cite{fitc-heteroskedasticity} found that the diagonal correlation $\text{diag}[V_{nn}]$ represents the posterior variances of $f(X_*)$ given $f(X_m)$ enables predictive variances to grow in regions far from inducing points, capturing a form of heteroskedasticity. However, Titsias \cite{vfe} found that approximate priors produce marginal likelihoods that are far from the marginal likelihood produced by the full prior, which degrades the accuracy of GPs trained using this goal. 

\subsubsection{Posterior approximations}

\paragraph{Variational free energy (VFE)}
Titsias \cite{vfe} introduced a variational distribution to approximate the noisy posterior:
\begin{equation*}
    q(f(X), f(X_m) | y) = p(f(X) | f(X_m)) \cdot q(f(X_m | y))
\end{equation*}
The Kullback-Leibler (KL) divergence measures how different two probability distributions are. The KL divergence between the true posterior $p(f(X), f(X_m) | y)$ and our variational distribution $q(f(X), f(X_m) | y)$ is:
\begin{equation*}
    D_{KL}(q(f(X), f(X_m) | y) || p(f(X), f(X_m) | y)) = \int q(f(X), f(X_m) | y) \log \frac{q(f(X), f(X_m) | y)}{p(f(X), f(X_m) | y)} df(X) df(X_m)
\end{equation*}
We need this expression in terms of $\log p(y)$ to form an approximation for the likelihood. We can introduce this term into $D_{KL}$ by Bayes' theorem:
\begin{equation*}
    D_{KL}(q(.) || p(.)) = \int q(f(X), f(X_m) | y) \log \frac{q(f(X), f(X_m) | y)}{p(y, f(X), f(X_m) | y) / p(y)} df(X) df(X_m)
\end{equation*}
Simplifying:
\begin{equation*}
    D_{KL}(q(.) || p(.)) = \int q(f(X), f(X_m) | y) \log \frac{q(f(X), f(X_m) | y) p(y)}{p(y, f(X), f(X_m) | y)} df(X) df(X_m)
\end{equation*}
Splitting the log term:
\begin{equation*}
    D_{KL}(q(.) || p(.)) = \int q(f(X), f(X_m) | y) \log \frac{q(f(X), f(X_m) | y)}{p(y, f(X), f(X_m) | y)} df(X) df(X_m) + \int q(f(X), f(X_m) | y) \log p(y) df(X) df(X_m)
\end{equation*}
$\log p(y)$ is a constant with respect to $f(X)$ and $f(X_m)$ and $\int q(f(X), f(X_m) | y) df(X) df(X_m) = 1$, so our second integral is just $\log p(y)$. Thus:
\begin{equation*}
    D_{KL}(q(.) || p(.)) = \int q(f(X), f(X_m) | y) \log \frac{q(f(X), f(X_m) | y)}{p(y, f(X), f(X_m) | y)} df(X) df(X_m) + \log p(y)
\end{equation*}
Rewriting to isolate $\log p(y)$:
\begin{equation} \label{eq:likelihood_vfe}
    \log p(y) = \int q(f(X), f(X_m) | y) \log \frac{p(y, f(X), f(X_m))}{q(f(X), f(X_m) | y)} df(X) df(X_m) + D_{KL}(q(.) || p(.)) 
\end{equation}

Our first term $\text{F} = \int q(f(X), f(X_m) | y) \log \frac{p(y, f(X), f(X_m))}{q(f(X), f(X_m) | y)} df(X) df(X_m)$ is called the Evidence Lower Bound (ELBO), or the VFE term. $D_{KL}(q(.) || p(.)) \geq 0$, so $\log p(y) \geq \text{F}$ and $\text{F}$ represents the lowest possible likelihood and raising $\text{F}$ produces a raise in $log(y)$. Maximising $\text{F}$ using the model hyperparameters directly improves the model, and maximising it using the parameters of the variational distribution makes it a better approximation which improves the hyperparameter tuning methods.

The resulting likelihood after computing the Gaussian integrals in \ref{eq:likelihood_vfe} is:
\begin{equation*}
    \begin{aligned}
        p_{\text{VFE}}(Y) = -\frac{1}{2} \log(2 \pi \sigma_n^2) \\ 
        - \frac{1}{2\sigma_n^2} Y^T Y \\
        + \frac{1}{2\sigma_n^4} Y^T K(X, X_m) [K(X_m, X_m) \\
        + \sigma_n^{-2} K(X_m, X) K(X, X_m)]^{-1} K(X_m, X) Y \\
        - \frac{1}{2} \log | K(X_m, X_m) + \sigma_n^{-2} K(X_m, X) K(X, X_m) | \\
        + \frac{1}{2} \log | K(X_m, X_m) | \\
        - \frac{1}{2 \sigma_n^2} \text{tr} \left[ K(X, X) - K(X_n, X_m) K(X_m, X_m)^{-1} K(X_m, X) \right]
    \end{aligned}
\end{equation*}
The trace term $\text{tr}[.]$ represents the residual variance the variational distribution has left unexplained. The terms in the likelihood that dominate computational complexity are the $y^T K(X, X_m) [K(X_m, X_m) + \sigma_n^{-2} K(X_m, X) K(X, X_m)]^{-1} K(X_m, X) y$ and the $K(X_m, X_m) + \sigma_n^{-2} K(X_m, X) K(X, X_m)$ inversions, which cost $O(nm^2)$ and $O(m^3)$ respectively, for an overall training complexity of $O(nm^2 + m^3)$.

For inference, this posterior produces the variational predictive distribution:
\begin{equation*}
    \begin{aligned}
        q(f(X_*)) = \mathcal{N}( \\
        K(X_*, X_m) K(X_m, X_m)^{-1} \mu^*, \\
        K(X_*, X_*) - K(X_*, X_m) K(X_m, X_m)^{-1} \Sigma^* K(X_m, X_m)^{-1} K(X_m, X_*) \\
        )
    \end{aligned}
\end{equation*}
$\mu^*$ and $\Sigma^*$ are the mean and covariance of the optimal $q(f(X_m) | y)$ variational distribution. This posterior approximation produces a likelihood where closed form solutions for $\mu^*$ and $\Sigma^*$ at $\partial \frac{p(y)}{\partial \mu} = 0$ and $\partial \frac{p(y)}{\partial \Sigma} = 0$ are available:
\begin{equation*}
    \begin{aligned}
        \Sigma^* = \left[ K(X_m, X_m)^{-1} + \sigma_n^{-2} K(X_m, X_m)^{-1} K(X_m, X) K(X, X_m) K(X_m, X_m)^{-1} \right]^{-1}
        \mu^* = \sigma_n^{-2} \Sigma^* K(X_m, X) y \\
    \end{aligned}
\end{equation*}
Assembling $\Sigma^*$ and $\mu^*$ requires inverting $K(X_m, X_m)$, which costs $O(m^3)$, and multiplying it by the $n$-rank $K(X_m, X)$ and $K(X, X_m)$, which costs $O(nm^2)$, for a total complexity of $O(nm^2 + m^3)$. VFE achieves a more accurate approximation of the marginal likelihood than FITC without additional computational complexity. 

\subparagraph{Deriving FITC through the VFE framework \cite{fitc-vfe-unifier}}
Bui et. al. \cite{fitc-vfe-unifier} found that applying expectation propagation, or minimising the "inclusive" KL where $p$ and $q$ are swapped: $D_{KL}(p(.) || q(.))$ recovers the FITC prior exactly:
\begin{equation*}
    \begin{aligned}
        q(f) = N ( \\
        0, \\
        K(X, X_m) K(X_m, X_m)^{-1} K(X_m, X) + \text{diag}[V_{nn}] \\
        )
    \end{aligned}
\end{equation*}
They defined the power expectation propagation (PEP) objective:
\begin{equation*}
    \log q_{\text{PEP}}(y) = \log q(y) - \frac{1 - \alpha}{2\alpha} \text{tr}\left[ \log(I_n + \frac{\alpha}{\sigma_n^2} V_{nn} \right]
\end{equation*}
$q(y)$ represents our FITC prior and the trace term represents $VFE$. When $\alpha = 1$, the trace term vanishes and we recover FITC exactly. As $\alpha \to 0$, the expansion of the $\frac{1-\alpha}/{\alpha} \log(I + \alpha A) \to tr[A]$ and we recover VFE in its collapsed form.  TODO improve explanation

\paragraph{Stochastic varational GP (SVGP)}
Hensman et. al. \cite{svgp} removed VFE's closed form solutions for the variational distribution's mean and variance and directly optimised them for lower computational complexity. They used a simple posterior approximation:
\begin{equation*}
    q(f(X_m) | y) = \mathcal{N}(f(X_m) | M, S)
\end{equation*}
$M$ and $S$ are the vector of means and the covariance matrix respectively, and are treated as free parameters to be optimised to maximise $F$. This produces a simpler ELBO:
\begin{equation*}
    F = \textbf{E}_{q(f(X))} \left[ \log p(y | f(X)) \right] - D_{KL}(q(f(X_m) | y) || p(f(X_m)))
\end{equation*}
The KL distance between our approximation and the prior is:
\begin{equation*}
    D_{KL}(q(f(X_m) | y) || p(f(X_m))) = \frac{1}{2} \left( \log \frac{|K(X_m, X_m)|}{|S} - m + \text{tr}(K(X_m, X_m)^{-1} S) + M^T K(X_m, X_m)^{-1} M \right)
\end{equation*}
We can use the posterior produced by these approximations to obtain $log p(y | f(X))$. The posterior is:
\begin{equation*}
    \begin{aligned}
        p(f(X) | X) = \mathcal{N}( \\
        K(X, X_m) K(X_m, X_m)^{-1} M, \\
        K(X, X) - K(X, X_m) K(X_m, X_m)^{-1} (K(X_m, X_m) - S) K(X_m, X_m)^{-1} K(X_m, X) \\
        )
    \end{aligned}
\end{equation*}
For the $i$-th point, we can find the squared difference between the true $y_i$ and the predicted $f(x_i)$ by producing predictions using the posterior mean $\mu_i$ and variance $\Sigma_{ii}$ at $i$:
\begin{equation*}
    \textbf{E}_{q(f(X_i))} \left[ \log p(y_i | f(X_i)) \right] =  \textbf{E} \left[ (y_i - f(x_i))^2 \right] = (y_i - \mu_i)^2 + \Sigma_{ii}
\end{equation*}
Assuming these remainders are Gaussian and independent of each other, we can sum all $i$ remainders together and plug them into the Gaussian likelihood to get a global expression:
\begin{equation*}
    \log p(y | f(X)) = -\frac{n}{2} \log(2 \pi \sigma_n^2) - \frac{1}{2\sigma_n^2} \sum_{i=1}^{n} \left[ (y_i - \mu_i)^2 + \Sigma{ii} \right]
\end{equation*}
The only reason both training and inference using this approximation is $O(mn^2 + m^3)$ is because the posterior means and variances currently cost $O(nm^2)$ to compute. Spending this during inference is unavoidable, but Stochastic Gradient Descent (SGD) \cite{adam} suggests that we can reduce training costs by only summing a batch of $b$ points at a time:
\begin{equation*}
    \log p(y | f(X)) = -\frac{n}{2} \log(2 \pi \sigma_n^2) - \frac{1}{2\sigma_n^2} \frac{n}{b} \sum_{i=1}^{b} \left[ (y_i - \mu_i)^2 + \Sigma{ii} \right]
\end{equation*}
This results in a training complexity of $O(bm^2 + m^3)$, which is significantly lower than the $O(nm^2 + m^3)$ complexity of VFE and FITC. TODO give full advantages \& disadvantages


% \subsubsection{Structured sparse approximation}

% \paragraph{Structured kernel interpolation (SKI) \cite{ski}}

% \subsubsection{Selecting inducing points}


% \subsection{Local approximations \cite{big-data}}
% 
% \subsubsection{Naive-local-experts}
% 
% \subsubsection{Mixture-of-experts}
% 
% \subsubsection{Product-of-experts}

\subsection{Celerite kernels \cite{foreman-mackay}}

\subsubsection{The celerite model}
Rybicki and Press \cite{fast-exp} used the Markov property of the exponential covariance function to produce a fast algorithm for computing its inverse. Since the exponential kernel is Markov, the only non-zero covariance at $i$ is $i-1$ and $i+1$, and the covariance matrix is tridiagonal whose inverse can be computed in $O(n)$ time. 

Kelly et. al. \cite{general-exp} generalised this to $J$ arbitrary mixtures of exponentials:
\begin{equation*}
    K(X, X') = \sum{j=1}^J a_j \exp(-c_j |X - X'|)
\end{equation*}
Although the inverse of this is dense, Kelly et. al. \cite{general-exp} showed that it can be computed in $O(N J^2)$ time.

Foreman and Mackay \cite{foreman-mackay} used the Bochner representation \ref{eq:bochner} of this generalised covariance function: %introduced complex parameters $a_j = a_j \pm i b_j$ and $c_j = c_j \pm i d_j$ to generalise this further :
\begin{equation*}
    \begin{aligned}
        K(X, X') = \sum_{j=1}^J \\
        frac{1}{2} (a_j + i b_j) \exp(-(c_j + i d_j) |X - X'|) \\
        + \frac{1}{2} (a_j - i b_j) \exp(-(c_j - i d_j) |X - X'|)
    \end{aligned}
\end{equation*}
Using the Fourier transform \ref{eq:fourier}:
\begin{equation} \label{eq:celerite}
    \begin{aligned}
        K(X, X') = \sum_{j=1}^J \\
        a_j \exp(-c_j |X - X'|) \cos(d_j |X - X'|) \\ 
        + b_j \exp(-c_j |X - X'|) \sin(d_j |X - X'|)
    \end{aligned}
\end{equation}
The argument within this sum is called a celerite term. Setting $b_j$ and $d_j$ recovers the original exponential kernel, but Foreman and Mackay's \cite{foreman-mackay} generalisation can be extended to approximate more complex kernels than sums of exponentials whilst maintaining Kelly et. al. \cite{general-exp}'s $O(N J^2)$ complexity.

\subsubsection{Building Matern 3/2 from celerite}
Applying the Fourier representation to the kernel to obtain its spectral density \ref{eq:spectral_density}:
\begin{equation*}
    S(\omega) = \sum_{j=1}^J \sqrt{\frac{2}{\pi}} \frac{(a_j c_j + b_j d_j) (c_j^2 + d_j^2) + (a_j c_j - b_j d_j) \omega^2}{\omega^4 + 2(c_j^2 - d_j^2) \omega^2 + (c_j^2 + d_j^2)^2}
\end{equation*}

A stochastically-driven dampened simple harmonic oscillator (SHO) is a system that oscillates with a frequency $\omega$ and is dampened by a factor $c$. The differential equation for this system is:
\begin{equation*}
    \left[ \frac{d^2}{dt^2} + \frac{\omega_0}{Q} \frac{d}{dt} + \omega_0^2 \right] y(t) = \eta(t)
\end{equation*}
Q is the "quality factor" of the oscillator, which determines how quickly it loses energy, and $\eta(t)$ is the stochastic force driving the oscillation. Anderson et. al. \cite{sho-spectral-density} found that if $\eta(t)$ is white noise, the spectral density of the SHO is:
\begin{equation*}
    S(\omega) = \sqrt{\frac{2}{\pi}} \frac{S_0 \omega_0^4}{(\omega^2 - \omega_0^2)^2 + \frac{\omega_0^2 \omega^2}{Q^2}}
\end{equation*}
where $S_0$ is proportional to the power at $\omega = \omega_0$, $S(\omega_0) = \sqrt{\frac{2}{\pi}} S_0 Q^2$ TODO explain.

Foreman and Mackay \cite{foreman-mackay} showed that the celerite spectral density matched the SHO spectral density if $Q \geq \frac{1}{2}$:
\begin{equation*}
    \begin{aligned}
        a_j = S_0 \omega_0 Q \\
        b_j = \frac{S_0 \omega_0 Q}{\sqrt{4Q^2 - 1}} \\
        c_j = \frac{\omega_0}{2Q} \\
        d_j = \frac{\omega_0}{2Q} \sqrt{4Q^2 - 1}
    \end{aligned}
\end{equation*}
For $0 < Q \leq \frac{1}{2}$, Foreman and Mackay \cite{foreman-mackay} used a pair of celerite terms with parameters:
\begin{equation*}
    \begin{aligned}
        a_{j \pm} = \frac{1}{2} S_0 \omega_0 Q \left[ 1 \pm \frac{1}{\sqrt{1 - 4Q^2}} \right] \\
        b_{j \pm} = 0 \\
        c_{j \pm} = \frac{\omega_0}{2Q} \left[ 1 \pm \sqrt{1 - 4Q^2} \right] \\
        d_{j \pm} = 0
    \end{aligned}
\end{equation*}
As $Q \to 0$, our spectral density indicates that the process has no oscillatory behaviour and represents a white noise process, e.g. $Q = 1 / \sqrt{2}$ recovers an astrophysics model \cite{sho-noise-astro} for background granulation noise in stars. Conversely, the process exhibits strong oscillatory behaviour as $Q \to 1$. 

We can use the Weiner-Khinchin theorem \ref{eq:weiner-khinchin} to obtain the SHO kernel from these spectral densities:
\begin{equation*}
    K(X, X'; S_0, Q, \omega_0) = S_0 \omega_0 Q \exp^{-\frac{\omega_0 |X - X'|}{2Q}} \begin{cases}
            \cosh (\eta \omega_0 |X - X'|) + \frac{1}{2 \eta Q} \sinh ( \eta \omega_0 |X - X'|) & 0 < Q < \frac{1}{2} \\
            2(1 + \omega_0 |X - X'|) & Q = \frac{1}{2} \\
            \cos (\eta \omega_0 |X - X'|) + \frac{1}{2 \eta Q} \sin ( \eta \omega_0 |X - X'|) & Q > \frac{1}{2}
    \end{cases}
\end{equation*}
$\eta = |1 - (4Q^2)^{-1} |^{1/2}$. 

At $Q = \frac{1}{2}$, the kernel becomes:
\begin{equation*}
    K(X, X')_{Q = \frac{1}{2}} = S_0 \omega_0 \exp^{- \omega_0 |X - X'|} \left( 1 + \omega_0 |X - X'| \right)
\end{equation*}
Setting $\omega_0 = \frac{\sqrt{3}}{l}$ recovers the Matern 3/2 kernel \label{eq:matern-32}.

\subsubsection{Cholesky factorisation and inversion of celerite kernels}
Foreman and Mackay \cite{foreman-mackay} develop a Cholesky factorisation method for these celerite kernels in $O(NR^2)$ complexity by exploiting their semiseperable structure. This methods reduces the $O(n^3)$ complexity of standard Cholesky factorisation \ref{eq:cholesky} whilst retaining its numerical stability.

\paragraph{Semiseperable matrices}
A rank-$R$ semiseperable matrix $K$ is a matrix that can be expressed as a sum of a diagonal matrix $A$ and two low-rank matrices $U$ and $V$:
\begin{equation} \label{eq:semiseperable-cases}
    K_ = \begin{cases}
        \sum_{r=1}^R U_{nr} V_{mr} & m < n \\
        A_{nn} & m = n \\
        \sum_{r=1}^R U_{mr} V_{nr} & m > n
    \end{cases}
\end{equation}
In matrix notation:
\begin{equation} \label{eq:semiseperable-matrix}
    K = A + \text{tri}_{\text{upper}}(U V^T) + \text{tri}_{\text{lower}}(V U^T)
\end{equation}
$A$ contains all the diagonal elements of the matrix $i = j$, and the second and third terms cover all the elements below and above the diagonal respectively.

\paragraph{Celerite kernels as semiseperable matrices}
For celerite kernels \ref{eq:celerite}, the diagonal component $A$ is simply:
\begin{equation*}
    A_{nn} = \sum_{j=1}^J a_j
\end{equation*} \cite{foreman-mackay}
For the off-diagonal components, defining $R = 2J$, we can assemble $U$ and $V$:
\begin{equation*}
    \begin{aligned}
        U_{n, 2j-1} = a_j \exp^{-c_j t_n} \cos(d_j t_n) + b_j \exp^{-c_j t_n} \sin(d_j t_n) \\
        U_{n, 2j} = a_j \exp^{-c_j t_n} \sin(d_j t_n) - b_j \exp^{-c_j t_n} \cos(d_j t_n) \\
        V_{m, 2j-1} = \exp^{+c_j t_m} \cos(d_j t_m) \\
        V_{m, 2j} = \exp^{+c_j t_m} \sin(d_j t_m)
    \end{aligned}
\end{equation*} \cite{foreman-mackay}
The resulting covariance matrix $K_{nm}$ can be assembled using the rules for semiseperable matrices \ref{eq:semiseperable}. 

\paragraph{Cholesky factorisation of semiseperable matrices}
The goal of Cholesky factorisation \ref{eq:cholesky} remains to find the square root of the matrix. For standard Cholesky factorisation, the diagonal is already baked into $L$. However, for semiseperable matrices the diagonal is not part of $L$ and needs to be taken into account in the Cholesky representation of $K$:
\begin{equation} \label{eq:cholesky-semiseperable}
    K = L D L^T
\end{equation}
Foreman and Mackay \cite{foreman-mackay} use the ansatz that our Cholesky factor has the form:
\begin{equation*}
    L = I + \text{tri}_{\text{lower}}(U W^T)
\end{equation*}
$I$ is the identity matrix, $U$ is from the definition of the semiseperable matrix \ref{eq:semiseperable-matrix}. After we determine the unknown $N \times N$ matrix $W$, we can determine our Cholesky factor. Substituting this ansatz into \ref{eq:cholesky-semiseperable}:
\begin{equation*}
    K = [I + \text{tri}_{\text{lower}}(U W^T)] D [I + \text{tri}_{\text{upper}}(W U^T)]
\end{equation*}
Simplifying:
\begin{equation*}
    K = D + \text{tri}_{\text{lower}}(U W^T) D + D \text{tri}_{\text{upper}}(W U^T) + \text{tri}_{\text{lower}}(U W^T) D \text{tri}_{\text{upper}}(W U^T)
\end{equation*}
Setting each element on the right-hand side equal to the corresponding element on the left-hand side of \ref{eq:semiseperable-cases} (e.g. $A = D$), we derive the following recurive definition W \cite{foreman-mackay}:
\begin{equation*}
    \begin{aligned}
        S_{n,j,k} = S_{n-1, j, k} + D_{n-1, n-1} W_{n-1,j} W_{n-1, k} \\
        D_{n,n} = A_{n,n} - \sum_{j=1}^{R} \sum_{k=1}^{R} U_{n,j} S_{n,j,k} U_{n,k} \\
        W_{n,j} = \frac{1}{D_{n,n}} \left[ V_{n,j} - \sum_{k=1}^{R} U_{n,k} S_{n,j,k} \right]
    \end{aligned}
\end{equation*}
Each iteration of this recursion costs $O(R^2)$, and this needs to be run for all $N$ rows in $W$ for an overall complexity of $O(N R^2)$. This cost dominates the $O(N)$ cost of obtaining the triangle matrix $U$ and the diagonal matrix $D$. 

\paragraph{Inversion of semiseperable matrices with a Cholesky factorisation}
As with standard regular Cholesky inversion, we can avoid the full cost of inversion since GPs only require the inverse multiplied by a product. This process is also cheaper than under regular Cholesky factorisation. As before, our goal is to solve $z = K^{-1} y$:
\begin{equation*}
    z = K^{-1} y = L^{T^{-1}} D^{-1} L^{-1} y
\end{equation*}
Recursively solving for $L^{-1} y$ using forward substitution, and defining the solution as $z'$:
\begin{equation*}
    \begin{aligned}
        f_{n,j} = f_{n-1,j} + W_{n-1,j} z'_{n-1} \\
        z'_n = y_n - \sum_{j=1}^{R} U_{n,j} f_{n,j}
    \end{aligned}
\end{equation*}
where $f_{0,j} = 0$ for all $j$. Using $z'$ and backward substitution, we can compute $z$ recursively:
\begin{equation*}
    \begin{aligned}
        g_{n,j} = g_{n+1,} + U_{n+1,j} z_{n+1} \\
        z_n = \frac{z'_n}{D_{n,n}} - \sum_{j=1}^R W_{n,j} g_{n,j}
    \end{aligned}
\end{equation*}
The total cost of both the forwards and backwards substitution is $O(N R)$.


% \subsubsection{Extensions}
% 
% \paragraph{Scalable manifold GP}
% 
% \paragraph{Scalable deep GP}
% 
% \paragraph{Scalable online GP}
% 
% \paragraph{Scalable multi-task GP}
% 
% \paragraph{Scalable recurrent GP}
